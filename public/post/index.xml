<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Yutong Dai / 戴宇童</title>
    <link>/post/</link>
    <description>Recent content in Posts on Yutong Dai / 戴宇童</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 -0600</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Anomaly Detection</title>
      <link>/post/anomaly-detection/</link>
      <pubDate>Fri, 04 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/anomaly-detection/</guid>
      <description>Point Anomaly Detection - Grubbs’ test Grubbs’ test1 is commonly used technique to detect an outlier in univariate problem, where normality assumption is required. It can be formualted as either one-side testing problem or two-sided testing problem.
The hypothesis test is defined as
\[H_0: \text{There are no outlier in the data set} \quad H_1: \text{There is exactly one outlier in the data set}.\] For two-sided testing, it tries to determine whether the observation with the largest absolute deviation is an outlier, where the test statistic is defined as</description>
    </item>
    
    <item>
      <title>Decision Tree: How to find the path from the root to the desired terminal node</title>
      <link>/post/decision-tree-how-to-find-the-path-from-the-root-to-the-desired-terminal-node/</link>
      <pubDate>Sat, 07 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/decision-tree-how-to-find-the-path-from-the-root-to-the-desired-terminal-node/</guid>
      <description>Prepare a fitted random forest Find the path to desired terminal node Collect Paths in the random forest Summarize the decison region   Prepare a fitted random forest import random import pandas as pd from sklearn.ensemble.forest import RandomForestRegressor from sklearn import tree data = pd.DataFrame({&amp;quot;Y&amp;quot;:[1,5,3,4,3,4,2], &amp;quot;X_1&amp;quot;:[&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;, &amp;quot;blue&amp;quot;, &amp;quot;red&amp;quot;,&amp;quot;red&amp;quot;,&amp;quot;blue&amp;quot;, &amp;quot;red&amp;quot;], &amp;quot;X_2&amp;quot;:[18.4, 7.5, 9.3, 3.7, 5.2, 3.2, 5.2]}) data = pd.get_dummies(data) X = data.drop([&amp;quot;Y&amp;quot;], axis=1) y = data[&amp;quot;Y&amp;quot;] rf = RandomForestRegressor(n_estimators = 10, random_state = 1234) rf.</description>
    </item>
    
    <item>
      <title>Fitting Linear Mixed Models</title>
      <link>/post/fitting-linear-mixed-models/</link>
      <pubDate>Tue, 12 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/fitting-linear-mixed-models/</guid>
      <description>Decide the random-effects covariance structure Case-Study IGF-I    Decide the random-effects covariance structure Use function lmList to have an idea of
which random-effects to include, and what the covariance structure to use.  library(nlme) # centering x to remove the correlation between intercept and slope orth.list &amp;lt;- lmList( distance ~ I(age-11) | Subject, data = Orthodont) # 1-id/2: significance level to identify the outlier pairs(orth.list, id = 0.</description>
    </item>
    
    <item>
      <title>The structure of grouped data</title>
      <link>/post/the-structure-of-grouped-data/</link>
      <pubDate>Fri, 08 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/the-structure-of-grouped-data/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Play with Sparse Matrix</title>
      <link>/post/play-with-sparse-matrix/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/play-with-sparse-matrix/</guid>
      <description>Motivation Solution Slicing Basic linear algebra operations    Motivation Recently, I have to deal with matrices with sparsity structures. If I coerce them using the as.matrix function, they consume my RAM wildly. So I turn to the package SparseM for possible solutions.
What I want to do are,
Performe basic linear algebra operations on sparse matrices, mainly including multiplication, transpose, and etc. Slice rows or columns by their indices.</description>
    </item>
    
    <item>
      <title>Get Data from the Website</title>
      <link>/post/get-data-from-website/</link>
      <pubDate>Thu, 31 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/get-data-from-website/</guid>
      <description>Motivation Solutions Download Uncompress data files Load libsvm format data Complete Code    Motivation I’m using the datasets libsvm to do some numerical experiments. The sizes of datasets, like epsilon1, are of severalGBs. So it is hard to play with the data on my desktop. I am lucky to run the experiments on a server. So my workflow is:
Download datasets on the server. Preprocess datastes. Run experiments.</description>
    </item>
    
    <item>
      <title>Bookdown Syntax</title>
      <link>/post/bookdown-syntax/</link>
      <pubDate>Mon, 28 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bookdown-syntax/</guid>
      <description>Math Equations with labels    Following contents are adapeted from Yihui Xie’s bookdown
 Math Equations with labels My personal preference is to use algin and equation for long or complicated and simple expressions respectively. The slight differeces, compared with the \(\bf \LaTeX\) syntax, are the way we assign labels to expressions. Instead of using \label{foo}, in blogdown (bookdown), it is recommend to use the syntax (\#eq:foo).</description>
    </item>
    
    <item>
      <title>Explore the Linear Mixed Effect Model</title>
      <link>/post/explore-the-linear-mixed-effect-model/</link>
      <pubDate>Fri, 16 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/explore-the-linear-mixed-effect-model/</guid>
      <description>Why Ramdom Effect Models One-way classification with Rail example Rail Data Modeling Fitting the model with the package nlme Diagnostic  Two-way classification: A Randomized Block Design egroStool: No Replications Modeling Diagnostic  Machines: Replications Modeling Model Selection  Extensions for interaction terms  ANCOVA with random effects: Orthodont Modeling: Random Intercept Modeling: Random Intercept Predicitons  Nested Classification Factors: Pixel Modeling  Split-plot    References: Pinheiro, J.</description>
    </item>
    
    <item>
      <title>Convergence Analysis for Block Coordinate Decent Algorithm and Powell&#39;s Examples</title>
      <link>/post/convergence-analysis-for-block-coordinate-descent-algorithm-and-powells-examples/</link>
      <pubDate>Thu, 17 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/convergence-analysis-for-block-coordinate-descent-algorithm-and-powells-examples/</guid>
      <description>Problem description Notations Assumption Algorithm  Convergence Analysis Powell’s example R codes for numerical experiments   We mainly focus on the convergence of Block coordinate decent with exact minimization, whose block update strategy employs Gauss-Seidel manner. And then use Powell’s example to see what will happen if some conditions are not met.
 Reference: 1. Dimitri .P Bertsekas, Nonlinear Programming 2ed 2. Powell ,1973, ON SEARCH DIRECTIONS FOR MINIMIZATION ALGORITHMS</description>
    </item>
    
  </channel>
</rss>