[{"authors":["admin"],"categories":null,"content":"I am currently a Ph.D. student in Lehigh\u0026rsquo;s Industrial and Systems Engineering (ISE) Department.\n","date":1548979200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1548979200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am currently a Ph.D. student in Lehigh\u0026rsquo;s Industrial and Systems Engineering (ISE) Department.","tags":null,"title":"Dai, Yutong","type":"authors"},{"authors":null,"categories":null,"content":"","date":1559260800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1559260800,"objectID":"395e2dc8a7bcb516f8e25f58c11ce4cc","permalink":"/resources/tex-rmd/","publishdate":"2019-05-31T00:00:00Z","relpermalink":"/resources/tex-rmd/","section":"resources","summary":"Templates for lecture notes, homework and cheatsheet.","tags":null,"title":"Description","type":"docs"},{"authors":null,"categories":null,"content":"","date":1559257200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559257200,"objectID":"c722bd58a2bce24ed42f2535fbdbebb6","permalink":"/resources/tex-rmd/cheatsheet/","publishdate":"2019-05-31T00:00:00+01:00","relpermalink":"/resources/tex-rmd/cheatsheet/","section":"resources","summary":"","tags":null,"title":"Cheatsheet Template","type":"docs"},{"authors":null,"categories":null,"content":"","date":1559257200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559257200,"objectID":"1af4d34cea320204a482ff65acc2cfbf","permalink":"/resources/tex-rmd/homework/","publishdate":"2019-05-31T00:00:00+01:00","relpermalink":"/resources/tex-rmd/homework/","section":"resources","summary":"","tags":null,"title":"Homework Template","type":"docs"},{"authors":null,"categories":null,"content":"","date":1559257200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559257200,"objectID":"1f506f6ecc9aa1be1d84803de7010339","permalink":"/resources/tex-rmd/lecturenotes/","publishdate":"2019-05-31T00:00:00+01:00","relpermalink":"/resources/tex-rmd/lecturenotes/","section":"resources","summary":"","tags":null,"title":"Lecture Notes Template","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":[],"categories":["Python-Programming"],"content":"# create virtual env with python 3.7.7, whose name is cuppy conda create -n cuppy numpy scipy pandas notebook matplotlib python=3.7.7 # activate cuppy conda activate cuppy # I am using zsh, you may change to bash conda init zsh # activate virtual env cond activate cuppy # point this verison of Python to jupyter ipython kernel install --name \u0026quot;cuppy\u0026quot; --user  ","date":1585353600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585430575,"objectID":"1e6c5efd96c1f3e42109f555e64b08cf","permalink":"/post/using-the-right-kernel-for-jupyter-notebook/","publishdate":"2020-03-28T00:00:00Z","relpermalink":"/post/using-the-right-kernel-for-jupyter-notebook/","section":"post","summary":"# create virtual env with python 3.7.7, whose name is cuppy conda create -n cuppy numpy scipy pandas notebook matplotlib python=3.7.7 # activate cuppy conda activate cuppy # I am using zsh, you may change to bash conda init zsh # activate virtual env cond activate cuppy # point this verison of Python to jupyter ipython kernel install --name \u0026quot;cuppy\u0026quot; --user  ","tags":["jupyter","kernel"],"title":"Using the right kernel for Jupyter Notebook","type":"post"},{"authors":null,"categories":["Optimization"],"content":" Some useful notes Farkas’s Lemma, Strong Duality and Criss-Cross Algorithm   ","date":1571356800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571356800,"objectID":"46aba06c97713885c14b80b492233241","permalink":"/post/ie406/lp/","publishdate":"2019-10-18T00:00:00Z","relpermalink":"/post/ie406/lp/","section":"post","summary":" Some useful notes Farkas’s Lemma, Strong Duality and Criss-Cross Algorithm   ","tags":["linear programming"],"title":"Introduction to Mathematical Programming","type":"post"},{"authors":[],"categories":["visualization"],"content":" 中国传统计时单位 古时一天分12个时辰，采用地支作为时辰名称，分为\n'子', '丑', '寅', '卯' '辰', '巳', '午', '未' '申', '酉', '戌', '亥'  时辰的起点是午夜，即子初。\n点击下图色块便可查看时间对应。 \n附录\n 参考文献:  国学网 新浪博客 博主知书少年果麦麦  用于绘图的Python代码  # using following code in jupyter notebook import plotly import plotly.graph_objs as go plotly.offline.init_notebook_mode(connected=True) colorPlate1 = [ '#ffb3a7', '#ffc773', '#ffa400', '#c9dd22', '#afdd22', '#cca4e3', '#b0a4e3', '#ffc64b', '#ffb61e', '#758a99', '#6b6882', '#ffb3a7', '#f47983', '#ffc773', '#ffa400', '#c9dd22', '#afdd22', '#cca4e3', '#b0a4e3', '#ffc64b', '#ffb61e', '#758a99', '#6b6882', '#f47983' ] colorPlate2 = ['#ff4e20', '#ff7500', '#789262', '#8d4bbb', '#e9bb1d','#50616d'] * 2 theta_marker = [\u0026quot;{}:00\u0026quot;.format(i) for i in range(24)] timeStamp = [ '子正', '丑初', '丑正', '寅初', '寅正', '卯初', '卯正', '辰初', '辰正', '巳初', '巳正', '午初', '午正', '未初', '未正', '申初', '申正', '酉初', '酉正', '戌初', '戌正', '亥初', '亥正', '子初' ] timeStampMain = [ '子, 名曰「困敦」\u0026lt;br\u0026gt;混沌万物之初萌，藏黄泉之下。\u0026lt;br\u0026gt; 子是兹的意思，这时候万物刚刚开始滋生和繁殖。', '丑, 名曰「赤奋若」\u0026lt;br\u0026gt;气运奋迅而起，万物无不若其性。\u0026lt;br\u0026gt;形容万物继续萌发，系于生长。', '寅, 名曰「摄提格」\u0026lt;br\u0026gt;万物承阳而起。\u0026lt;br\u0026gt;植物芽刚刚吐露，要吸收阳气生长，然后全部露出地面。', '卯, 名曰「单阏」\u0026lt;br\u0026gt;阳气推万物而起. \u0026lt;br\u0026gt;卯，就是茂，茂盛的样子。这个时候，万物生长滋生繁茂。', '辰, 名曰「执徐」\u0026lt;br\u0026gt;伏蛰之物，而敷舒出。\u0026lt;br\u0026gt;万物都震动而生长，草木伸舒，萌芽而出。', '巳, 名曰「大荒落\u0026lt;br\u0026gt;万物炽盛而出，霍然落之。\u0026lt;br\u0026gt;万物到了这个时候，都全部长起来了，聚集在一起。炽盛而有光泽的样子。', '午, 名曰「敦牂」\u0026lt;br\u0026gt;万物壮盛也。\u0026lt;br\u0026gt;万物都达到盛大壮茂，枝柯密布的状态。', '未, 名曰「协洽」\u0026lt;br\u0026gt;阴阳和合，万物化生。\u0026lt;br\u0026gt;未，就是味的意思。当事物成熟的时候，都会发出气味。这时候阴气开始升起，万物稍微衰败。' , '申, 名曰「涒滩」\u0026lt;br\u0026gt;万物吐秀，倾垂也。\u0026lt;br\u0026gt;万物的身体都已成就，倾吐了最后的繁盛，引向衰败。', '酉, 名曰「作噩」\u0026lt;br\u0026gt;万物皆芒枝起。 \u0026lt;br\u0026gt;万物衰老到极至而成熟。', '戌, 名曰「阉茂」\u0026lt;br\u0026gt;万物皆蔽冒也。\u0026lt;br\u0026gt;戌，灭，杀的意思。意思是到了这时候，万物都已经衰灭了。', '亥, 名曰「大渊献」\u0026lt;br\u0026gt;万物于天，深盖藏也。\u0026lt;br\u0026gt;亥，核的意思。万物都进入核阂里，意味着阴气劾杀了万物，等待下一个初萌。' ] def sectorChild(name, location, radius, fillcolor='#ff4e20'): r = [0] * 24 r[location % 24] = radius r[(location + 1) % 24] = radius obj = go.Scatterpolar( name = name, r = r, theta = theta_marker, fill = \u0026quot;toself\u0026quot;, fillcolor = fillcolor, line = {\u0026quot;color\u0026quot;:'black'} ) return obj def sectorParent(name, location, radius, fillcolor='black'): r = [0] * 24 if location == 0: r[0] = r[1] = r[23] = radius else: r[location % 24] = radius r[(location + 1) % 24] = radius r[(location + 2) % 24] = radius obj = go.Scatterpolar( name = name, r = r, theta = theta_marker, fill = \u0026quot;toself\u0026quot;, fillcolor = fillcolor, line = {\u0026quot;color\u0026quot;:'black'}, hoverinfo = 'text', hoverlabel = {'align':'left'} ) return obj trace = [sectorChild(i, j, 5, k) for (i,j,k) in zip(timeStamp, range(24), colorPlate1)] trace += [sectorParent(timeStampMain[0], 0, 3, colorPlate2[0])] trace += [sectorParent(i, j, 3, k) for (i,j, k) in zip(timeStampMain[1:], range(1, 23, 2), colorPlate2[1:]) ] layout = go.Layout( polar = dict( radialaxis = dict( visible = False ), angularaxis = dict( direction = \u0026quot;clockwise\u0026quot;, visible = True, linewidth = 3 ) ), showlegend = False ) plotly.offline.iplot({ \u0026quot;data\u0026quot;: trace, \u0026quot;layout\u0026quot;: layout })  ","date":1562284800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562313530,"objectID":"a78143e43dd46b5640ab9af8fd0b0dd5","permalink":"/post/chinese-clock/chinese-time/","publishdate":"2019-07-05T00:00:00Z","relpermalink":"/post/chinese-clock/chinese-time/","section":"post","summary":"中国传统计时单位 古时一天分12个时辰，采用地支作为时辰名称，分为\n'子', '丑', '寅', '卯' '辰', '巳', '午', '未' '申', '酉', '戌', '亥'  时辰的起点是午夜，即子初。\n点击下图色块便可查看时间对应。 \n附录\n 参考文献:  国学网 新浪博客 博主知书少年果麦麦  用于绘图的Python代码  # using following code in jupyter notebook import plotly import plotly.graph_objs as go plotly.offline.init_notebook_mode(connected=True) colorPlate1 = [ '#ffb3a7', '#ffc773', '#ffa400', '#c9dd22', '#afdd22', '#cca4e3', '#b0a4e3', '#ffc64b', '#ffb61e', '#758a99', '#6b6882', '#ffb3a7', '#f47983', '#ffc773', '#ffa400', '#c9dd22', '#afdd22', '#cca4e3', '#b0a4e3', '#ffc64b', '#ffb61e', '#758a99', '#6b6882', '#f47983' ] colorPlate2 = ['#ff4e20', '#ff7500', '#789262', '#8d4bbb', '#e9bb1d','#50616d'] * 2 theta_marker = [\u0026quot;{}:00\u0026quot;.","tags":["visualization"],"title":"中国传统计时","type":"post"},{"authors":[],"categories":["optimization"],"content":" Table of Contents HAHAHUGOSHORTCODE-TOC0-HBHB\nProblem Formulation There are commonly two ways of formulating the logistic regression problem, depending on the way we label the response variable $y$. Here we focus on the first formulation and defer the second formulation on the appendix.\nFirst Formulation:\nConsider restrict $y$ to {${-1,1}$}. Then we have $$ \\begin{aligned} \u0026amp;\\mathbb{P}(y=1|z)=\\sigma(z)=\\frac{1}{1 + e^{-z}}\n\u0026amp;\\mathbb{P}(y=-1|z)=\\sigma(z)=\\frac{1}{1 + e^z}, \\end{aligned} $$ which can be compactly written as $$ \\mathbb{P}(y|z)=\\sigma(zy). $$ If we consider the data $({x_i,y_i})_{i=1}^N$ and we want to use the Likelihood Principle to fit the Logistic Regression, then we would like to maximize the following loss function, $$ \\begin{aligned} \u0026amp; L(\\beta_0,\\beta) = \\prod_{i=1}^N \\mathbb{P}(y_i|z_i)\n\u0026amp; z_i =\\beta_0+\\beta^Tx_i. \\end{aligned} $$ If we use the first formulation, then it is equivalent to minimize the log-negative of $L(\\beta_0,\\beta)$, $$ \\begin{aligned} \\min_{\\beta_0,\\beta}l(\\beta_0,\\beta)=\\frac{1}{N}\\sum_{i=1}^N\\log(1+e^{-y_iz_i}). \\end{aligned} $$ From now on, for the sake of simplicity, we drop the intercept term $\\beta_0$.\nMotivating Example Consider two simulated datasets:\nDataset 1:\n   $x_1$ $x_2$ $y$     0.3 0.9 1   0.5 1.5 -1    Dataset 2:\n   $x_1$ $x_2$ $y$     2 1 1   -1 -1 -1    Some Analysis:\n The objective function $l(\\beta)$ is strictly convex by looking at its Hessian, which is positive defined. However, it is not strongly convex. For given Data set, the Hessian is upper bounded by $(\\sum_{i=1}^N|x_i|^2)I$ (see Appendix). The stepsize can be chosen as $\\alpha = \\frac{1}{\\sum_{i=1}^N|x_i|^2}$.  Applying the gradient descent with constant stepsize $\\frac{1}{L}$ on each dataset for 1000 steps, then we obtain the estimations as follows.\n   Dataset $\\beta_1$ $\\beta_2$     1 -0.12058225 -0.36174676   2 3.59370507 3.04825501    Also we plot out following figures to check the convergence. The top two figures describe the algorithm\u0026rsquo;s performance on the dataset 1 while the bottom two is for the dataset 2.\n Fig1: Apply GD with the constant stepsize on two different datasets. The blue curves depicts how the norm of gradient at iterates change while the red curves show the change of the function value in each iteration.\n Analysis: why this happens? First, if we want to minimize $f(\\beta)=\\log(1 + \\exp(-\\beta))$ using gradient descent with constant stepsize $\\frac{1}{L}$, then we will facing following issues. Here we assume $\\beta \\in \\mathbb{R}$.\n The global minimal is not attainable, i.e., $+\\infty$, though we can have $\\nabla f(\\beta^k)\\rightarrow 0$, which means $\\beta \\rightarrow +\\infty$, hence the iterates diverge. Indeed, the ${f(\\beta^k)}$ converges to $f^*=0$ by as it monotonously decreasing and lower bounded by $0$. The worst-case iteration complexity is $\\mathcal{O}(\\frac{1}{k})$, indicating a sublinear convergence rate.  Now, let\u0026rsquo;s back to the example. The figure 2 shows that the first dataset and second dataset, which correspond to the non-separable and separable case respectively.\n Fig2: (Left) First dataset. (Right) Second dataset. The fitted separating line is derived by $y=-\\frac{\\beta_1}{\\beta_2}x$.\n We also plot out the norm of iterates at each iteration in figure 3.\n Fig3: The top figure shows the norm of iterates for the first dataset while the bottom one shows case for the second dataset.\n We can see that for non-separable case, the norm of iterates are bounded while the latter goes to infinity (if we increase the number of iterations).\nIn non-separable case, ${\\beta^k}$ seems to stay in \u0026ldquo;strongly convex\u0026rdquo; region while in separable case, ${\\beta^k}$ keeps approaching the flatten region, so you can easily say a sharp decreasing in convergence speed. The following observations can be verified by figure 4 (1-dimensional case) and figure 5 (2-dimensional case).\n Fig4: (Left) Non-separable dataset ${(x_1=1, y_1=1), (x_2=2, y_2=-1)}$. The green dot line is $y=x^2$. The objective function (blue line) preserves the strong convexity in a certain range and the minimal stays in this range. The red point is the start point $x_0$. (Right). Separable dataset ${(x_1=1, y_1=1), (x_2=-1, y_2=-1)}$. Although the objective function is endowed with the strong convexity property in a certain range, however the global minimal is outside of this range.\n  Fig5: Dataset 1 is shown in the top 2 pictures with the right one zooming into a particular range. Dataset 2 is shown in the bottom pictures. The blue dots trace the progression of iterates.\n Questions  Why the separability would cause such a difference? From the Fig4 and Fig5, we know data as parameters can influence the shape of the objective function a lot. Given the data set, can we predict the behavior of the performance of gradient descent with constant stepsize, i.e., linear convergence rate or sublinear convergence rate? Can we extend our conclusion to higher dimension? In real world application, it\u0026rsquo;s likely that the data is semi-separable, i.e., most data points can be split into two groups with a few exceptions. How\u0026rsquo;s that influence the performance of the algorithm? Will second formulation (see below) also encounter the similar issue? My guess is yes.  Appendix Second Formulation of Logistic Regression\nConsider restrict $y$ to ${0,1}$. Then we have $$ \\begin{aligned} \u0026amp;\\mathbb{P}(y=1|z)=\\sigma(z)=\\frac{1}{1 + e^{-z}}\n\u0026amp;\\mathbb{P}(y=0|z)=\\sigma(z)=\\frac{1}{1 + e^z}, \\end{aligned} $$ which can be compactly written as $$ \\mathbb{P}(y|z)=\\sigma(z)^y(1-\\sigma(z))^{1-y}. $$\nIf we use the second formulation, then maximizing the likelihood is equivalent to $$ \\begin{aligned} \\min_{\\beta_0,\\beta}l(\\beta_0,\\beta)=\\frac{1}{N}\\sum_{i=1}^N[-y_iz_i+\\log(1+e^{z_i})]. \\end{aligned} $$\nDerivation of the gradient and Hessian of the loss function (first formualtion)\nConsider $f(\\beta)=\\log (1 + \\exp(-y\\beta^Tx)$, then we have $$ \\begin{aligned} \u0026amp;\\nabla f(\\beta) = \\frac{1}{1 + \\exp(y\\beta^Tx)}(-yx)\n\u0026amp;\\nabla^2 f(\\beta) = (yx)\\frac{\\exp(y\\beta^Tx)}{1 + \\exp(y\\beta^Tx)}(yx^T), \\end{aligned} $$ which implies $$ \\begin{aligned} \u0026amp; \\nabla l(\\beta)=\\frac{1}{N}\\sum_{i=1}^N \\frac{1}{1 + \\exp(y_i\\beta^Tx_i)}(-y_ix_i)\n\u0026amp; \\nabla^2 l(\\beta)=\\frac{1}{N}\\sum_{i=1}^N(y_ix_i)\\frac{\\exp(y\\beta^Tx_i)}{(1 + \\exp(y\\beta^Tx_i))^2}(y_ix_i^T)=\\frac{1}{N}XDX^T, \\end{aligned} $$ where $X=[x_1,\\cdots,x_n]$ , $D=\\text{diag}({y_1^2\\sigma_1(1-\\sigma_1),\\cdots,y_n^2\\sigma_n(1-\\sigma_n)})$ , and $\\sigma_i =\\frac{\\exp(y\\beta^Tx_i)}{1 + \\exp(y\\beta^Tx_i)} $.\nReference:\n Lecture notes 9 and 10 presented on this course website.\n The code for generating graphs can be found in my git repo.\n  ","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561967930,"objectID":"473ae1c2728e1a3326fc148defe204ad","permalink":"/post/gdlogistic/gradient-descent-in-logistic-regression/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/post/gdlogistic/gradient-descent-in-logistic-regression/","section":"post","summary":"Table of Contents HAHAHUGOSHORTCODE-TOC0-HBHB\nProblem Formulation There are commonly two ways of formulating the logistic regression problem, depending on the way we label the response variable $y$. Here we focus on the first formulation and defer the second formulation on the appendix.\nFirst Formulation:\nConsider restrict $y$ to {${-1,1}$}. Then we have $$ \\begin{aligned} \u0026amp;\\mathbb{P}(y=1|z)=\\sigma(z)=\\frac{1}{1 + e^{-z}}\n\u0026amp;\\mathbb{P}(y=-1|z)=\\sigma(z)=\\frac{1}{1 + e^z}, \\end{aligned} $$ which can be compactly written as $$ \\mathbb{P}(y|z)=\\sigma(zy).","tags":[],"title":"Gradient Descent in Logistic Regression","type":"post"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Dai, Yutong","Weng, Yang"],"categories":null,"content":"","date":1548979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548979200,"objectID":"f934a9b2b0fdb05ac23f1c7f653ed912","permalink":"/publication/psum/","publishdate":"2019-02-01T00:00:00Z","relpermalink":"/publication/psum/","section":"publication","summary":"In this paper, we propose a synchronous parallel block coordinate descent algorithm(PSUM) for minimizing a composite function, which consists of a smooth convex function plus a non-smooth but separable convex function. Due to the generalization of our method, some existing synchronous parallel algorithms can be considered as special cases. To tackle high dimensional problems, we further develop a randomized variant, which randomly update some blocks of coordinates at each round of computation. Both proposed parallel algorithms are proven to have sub-linear convergence rate under rather mild assumptions. The numerical experiments on solving the large scale regularized logistic regression with $l_1$ norm penalty show that the implementation is quite efficient. We conclude with explanation on the observed experimental results and discussion on the potential improvements.","tags":["Convex Optimization","Block Cooridinate Descent"],"title":"(To appear.) Synchronous Parallel Block Coordinate Descent Method for Nonsmooth Convex Function Minimization","type":"publication"},{"authors":null,"categories":["Statistics"],"content":" Point Anomaly Detection - Grubbs’ test Grubbs’ test1 is commonly used technique to detect an outlier in univariate problem, where normality assumption is required. It can be formualted as either one-side testing problem or two-sided testing problem.\nThe hypothesis test is defined as\n\\[H_0: \\text{There are no outlier in the data set} \\quad H_1: \\text{There is exactly one outlier in the data set}.\\] For two-sided testing, it tries to determine whether the observation with the largest absolute deviation is an outlier, where the test statistic is defined as\n\\[ G = \\frac{\\max_i |X_i - \\bar X|}{s}, \\] where the \\(\\bar X\\) is the sample mean and \\(s\\) is the sample deviation.\nLet’s look at one simulated example.\nset.seed(123) simulated_data \u0026lt;- rnorm(100, 0, 1) simulated_data_with_outliers \u0026lt;- c(simulated_data, c(3.5, -3.7)) # normality check shapiro.test(simulated_data_with_outliers) ## ## Shapiro-Wilk normality test ## ## data: simulated_data_with_outliers ## W = 0.9804, p-value = 0.1344 Shapiro-Wilk normality test impiles the data is normally distributed. Now, let’s performe the grubbs’ test.\nlibrary(outliers) grubbs.test(simulated_data_with_outliers, two.sided = TRUE) ## ## Grubbs test for one outlier ## ## data: simulated_data_with_outliers ## G = 3.65380, U = 0.86651, p-value = 0.01626 ## alternative hypothesis: lowest value -3.7 is an outlier The test result detecs the lowest value as an outlier.\nLet’s remove the -3.7 and performe the test again.\ngrubbs.test(head(simulated_data_with_outliers,101), two.sided = TRUE) ## ## Grubbs test for one outlier ## ## data: head(simulated_data_with_outliers, 101) ## G = 3.48190, U = 0.87755, p-value = 0.03378 ## alternative hypothesis: highest value 3.5 is an outlier Finally, let’s remove two outliers altogether.\ngrubbs.test(head(simulated_data_with_outliers,100), two.sided = TRUE) ## ## Grubbs test for one outlier ## ## data: head(simulated_data_with_outliers, 100) ## G = 2.62880, U = 0.92949, p-value = 0.7584 ## alternative hypothesis: lowest value -2.30916887564081 is an outlier This impiles there are no outliers.\nGrubbs’ test is useful for identify the outliers of a small amount one at a time, but not suitable to detect a group of outliers.\n Collective Anomaly Detection Anomaly in timeseries - Seasonal Hybrid ESD algorithm # devtools::install_github(\u0026quot;twitter/AnomalyDetection\u0026quot;) library(AnomalyDetection) river \u0026lt;- read.csv(\u0026quot;https://raw.githubusercontent.com/Rothdyt/personal-blog/master/static/files/post_data/river.csv\u0026quot;) results \u0026lt;- AnomalyDetectionVec(river$nitrate, period=12, direction = \u0026#39;both\u0026#39;, plot = T) results$plot  Distance-based Anomaly Detection Global Anomaly - Largest Distance Intuitively, the larger distance the more likely the point would be an outlier.\nlibrary(FNN) ## Warning: package \u0026#39;FNN\u0026#39; was built under R version 3.4.4 furniture \u0026lt;- read.csv(\u0026quot;https://raw.githubusercontent.com/Rothdyt/personal-blog/master/static/files/post_data/furniture.csv\u0026quot;) furniture_scaled \u0026lt;- data.frame(Height = scale(furniture$Height), Width = scale(furniture$Width)) furniture_knn \u0026lt;- get.knn(furniture_scaled, k = 5) furniture_scaled$score_knn \u0026lt;- rowMeans(furniture_knn$nn.dist) largest_idx \u0026lt;- which.max(furniture_scaled$score_knn) plot(furniture_scaled$Height, furniture_scaled$Width, cex=sqrt(furniture_scaled$score_knn), pch=20) points(furniture_scaled$Height[largest_idx], furniture_scaled$Width[largest_idx], col=\u0026quot;red\u0026quot;, pch=20)  Local Anomaly - LOF kNN is useful for finding global anomalies, but is less able to surface local outliers.\nLOF is a ratio of densities:\n LOF \u0026gt; 1 more likely to be anomalous LOF ≤ 1 less likely to be anomalous Large LOF values indicate more isolated points  library(dbscan) furniture_lof \u0026lt;- furniture[,2:3] furniture_lof$score_lof \u0026lt;- lof(scale(furniture_lof), k=5) largest_idx \u0026lt;- which.max(furniture_lof$score_lof) plot(furniture_lof$Height, furniture_lof$Width, cex=sqrt(furniture_lof$score_lof), pch=20) points(furniture_lof$Height[largest_idx], furniture_lof$Width[largest_idx], col=\u0026quot;red\u0026quot;, pch=20) It’s clear that, lof successfuly detects the local outlier.\n   Isolation Forest  Isolation Forest is built on the basis of decision trees; To grow a decision tree, at each node, a feature and a corresponding cutoff value are randomly selected; Intuitively, outliers are less frequent than regular observations and are different from them in terms of values, so outliers should be identified closer to the root of the tree with fewer splits. We use isolation score to characterize this.  Isolation Score We need some quatity to define the isolation score2\n Path Length: \\(h(x)\\) of a point \\(x\\) is measured by the number of edges \\(x\\) traverses an iTree from the root node until the traversal is terminated at an external node. Normalizing constant \\[c(n) = 2H(n − 1) − (2(n − 1)/n)\\], where \\(n\\) is the number of samples to grow a tree and \\(H(i)\\) is the harmonic number and it can be estimated by \\(ln(i) + 0.5772156649\\) (Euler’s constant).  The isolation score \\(s\\) of an sample \\(x\\) is defined as \\[s(x,n)= 2^{-\\frac{E(h(x))}{c(n)}},\\] where the \\(E()\\) is the expectation of \\(h(x)\\).\n Interpreting the isolation score:\n Scores between 0 and 1 Scores near 1 indicate anomalies (small path length)\n  # devtools::install_github(\u0026quot;Zelazny7/isofor\u0026quot;) library(isofor) furniture \u0026lt;- read.csv(\u0026quot;https://raw.githubusercontent.com/Rothdyt/personal-blog/master/static/files/post_data/furniture.csv\u0026quot;) furniture \u0026lt;- data.frame(Height = furniture$Height, Width = furniture$Width) scores \u0026lt;- matrix(nrow=dim(furniture)[1]) for (ntree in c(100, 200, 500)){ furniture_tree \u0026lt;- iForest(furniture, nt = ntree, phi=50) scores \u0026lt;- cbind(scores, predict(furniture_tree, furniture)) } plot(scores[,3], scores[,4], xlab = \u0026quot;200 tress\u0026quot;, ylab=\u0026quot;500 tress\u0026quot;) abline(a=0,b=1) This graph is used to assess wheter the number of trees is enough for the isolation score to converge. From the graph above, we know that 200 tress are enough for us to identify the anomalies.\nlibrary(lattice) furniture_forest \u0026lt;- iForest(furniture, nt = 200, phi=50) h_seq \u0026lt;- seq(min(furniture$Height), max(furniture$Height), length.out = 20) w_seq \u0026lt;- seq(min(furniture$Width), max(furniture$Width), length.out = 20) furniture_grid \u0026lt;- expand.grid(Width = w_seq, Height = h_seq) furniture_grid$score \u0026lt;- predict(furniture_forest, furniture_grid) contourplot(score ~ Height + Width, data = furniture_grid,region = TRUE) This contour graph used to identify the anomaly regions.\n   https://en.wikipedia.org/wiki/Grubbs%27_test_for_outliers↩\n Zhihua Zhou et al. https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf↩\n   ","date":1546560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546560000,"objectID":"0f9c92b28639dcbc1da16b2bc8ac9897","permalink":"/post/anomaly-detection/anomaly-detection/","publishdate":"2019-01-04T00:00:00Z","relpermalink":"/post/anomaly-detection/anomaly-detection/","section":"post","summary":"Point Anomaly Detection - Grubbs’ test Grubbs’ test1 is commonly used technique to detect an outlier in univariate problem, where normality assumption is required. It can be formualted as either one-side testing problem or two-sided testing problem.\nThe hypothesis test is defined as\n\\[H_0: \\text{There are no outlier in the data set} \\quad H_1: \\text{There is exactly one outlier in the data set}.\\] For two-sided testing, it tries to determine whether the observation with the largest absolute deviation is an outlier, where the test statistic is defined as","tags":["data-analysis"],"title":"Anomaly Detection","type":"post"},{"authors":null,"categories":["Python-Programming"],"content":"  Prepare a fitted random forest Find the path to desired terminal node Collect Paths in the random forest Summarize the decison region   Prepare a fitted random forest import random import pandas as pd from sklearn.ensemble.forest import RandomForestRegressor from sklearn import tree data = pd.DataFrame({\u0026quot;Y\u0026quot;:[1,5,3,4,3,4,2], \u0026quot;X_1\u0026quot;:[\u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;, \u0026quot;blue\u0026quot;, \u0026quot;red\u0026quot;,\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;, \u0026quot;red\u0026quot;], \u0026quot;X_2\u0026quot;:[18.4, 7.5, 9.3, 3.7, 5.2, 3.2, 5.2]}) data = pd.get_dummies(data) X = data.drop([\u0026quot;Y\u0026quot;], axis=1) y = data[\u0026quot;Y\u0026quot;] rf = RandomForestRegressor(n_estimators = 10, random_state = 1234) rf.fit(X, y) output:\nRandomForestRegressor(bootstrap=True, criterion=\u0026#39;mse\u0026#39;, max_depth=None, max_features=\u0026#39;auto\u0026#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1, oob_score=False, random_state=1234, verbose=0, warm_start=False)  Find the path to desired terminal node import pydotplus import re def return_node_path_to_max_prediction(onetree, verbose=True): \u0026quot;\u0026quot;\u0026quot; @input: a tree from the sklearn randomforest @output: the node path to maxmium terminal node [[split_node_1], [split_node_2], ...] [splite_node_1] = [var_index, cutoff, direction] \u0026quot;\u0026quot;\u0026quot; if verbose: print(\u0026quot;Generating Tree Graph, it may take a while...\u0026quot;) dot_data = tree.export_graphviz(onetree, out_file = None, filled = True, rounded = True, special_characters = True) graph = pydotplus.graph_from_dot_data(dot_data) graph_ = {} for edge in graph.get_edge_list(): graph_[edge.get_source()] = edge.get_destination() # find all terminal node terminal_node = {} non_decimal = re.compile(r\u0026#39;[^\\d.]+\u0026#39;) for node in graph.get_node_list(): if node.get_name() not in graph_: if node.get_name() not in [\u0026quot;node\u0026quot;, \u0026quot;edge\u0026quot;]: value = node.get_label() value = re.sub(r\u0026#39;.*v\u0026#39;, \u0026#39;v\u0026#39;, value) terminal_node[node.get_name()] = float(non_decimal.sub(\u0026#39;\u0026#39;, value)) # find the path down to the terminal with maximum predition value flag = True destination = max(terminal_node, key=terminal_node.get) edge_list = graph.get_edge_list() node_list = graph.get_node_list() split_node = [] while flag: myedge = [edge for edge in edge_list if edge.get_destination() == destination][0] if int(myedge.get_destination()) - int(myedge.get_source()) \u0026gt; 1: direction = \u0026quot;Right\u0026quot; else: direction = \u0026quot;Left\u0026quot; mynode = [node for node in node_list if node.get_name() == myedge.get_source()][0] var_val = re.findall(r\u0026quot;[-+]?\\d*\\.\\d+|\\d+\u0026quot;, mynode.get_label())[:2] # record the growing path: # var_val[0]: Index of variable participating in splitting # var_val[1]: cutoff point of the splitting # direction: If Right, means greater than var_val[1]; # If Left, means no greater than var_val[1] split_node.append([int(var_val[0]),float(var_val[1]),direction]) if verbose: print(myedge.get_destination() + \u0026quot;\u0026lt;-\u0026quot; + myedge.get_source() + \u0026quot;: Split at Variable X\u0026quot; + var_val[0] + \u0026quot;; The cutoff is \u0026quot; + var_val[1] + \u0026quot;; Turn \u0026quot; + direction) destination = myedge.get_source() if destination == \u0026quot;0\u0026quot;: flag = False return [*reversed(split_node)] Test:\nreturn_node_path_to_max_prediction(rf[1], verbose=True) Outputs:\nGenerating Tree Graph, it may take a while... 3\u0026lt;-1: Split at Variable X0; The cutoff is 5.6; Turn Right 1\u0026lt;-0: Split at Variable X0; The cutoff is 12.95; Turn Left From the output above, we know the path from the root to the desired terminal node is :\nRoot[X0(\u0026lt;= 12.95)] -\u0026gt; X0 (\u0026gt;=5.6) -\u0026gt; Terminal Node\n Collect Paths in the random forest def collect_path(rf, verbose=True): n_tree = len(rf) result = [] for i in range(n_tree): if verbose: print(\u0026quot;Construct the %s tree graph out of %s trees\u0026quot; %(i+1, n_tree)) result.append(return_node_path_to_max_prediction(rf.estimators_[i], verbose=False)) return result Test:\nresult = collect_path(rf) print(result) Outputs:\nConstruct the 1 tree graph out of 10 trees Construct the 2 tree graph out of 10 trees Construct the 3 tree graph out of 10 trees Construct the 4 tree graph out of 10 trees Construct the 5 tree graph out of 10 trees Construct the 6 tree graph out of 10 trees Construct the 7 tree graph out of 10 trees Construct the 8 tree graph out of 10 trees Construct the 9 tree graph out of 10 trees Construct the 10 tree graph out of 10 trees [[[0, 4.2, \u0026#39;Left\u0026#39;]], [[0, 12.95, \u0026#39;Left\u0026#39;], [0, 5.6, \u0026#39;Right\u0026#39;]], [[1, 0.5, \u0026#39;Right\u0026#39;], [0, 8.4, \u0026#39;Left\u0026#39;]], [[0, 13.85, \u0026#39;Left\u0026#39;], [0, 8.4, \u0026#39;Left\u0026#39;], [1, 0.5, \u0026#39;Right\u0026#39;]], [[0, 8.4, \u0026#39;Left\u0026#39;], [0, 6.35, \u0026#39;Right\u0026#39;]], [[0, 12.95, \u0026#39;Left\u0026#39;], [0, 5.6, \u0026#39;Right\u0026#39;]], [[2, 0.5, \u0026#39;Left\u0026#39;], [0, 5.35, \u0026#39;Right\u0026#39;]], [[1, 0.5, \u0026#39;Right\u0026#39;], [0, 5.35, \u0026#39;Right\u0026#39;]], [[0, 13.85, \u0026#39;Left\u0026#39;], [1, 0.5, \u0026#39;Right\u0026#39;], [0, 8.4, \u0026#39;Left\u0026#39;], [0, 5.35, \u0026#39;Right\u0026#39;]], [[0, 13.85, \u0026#39;Left\u0026#39;], [0, 6.35, \u0026#39;Right\u0026#39;], [0, 8.4, \u0026#39;Left\u0026#39;]]]  Summarize the decison region def summarize_region(result, features): decision_region = {k: [[] for _ in range(2)] for k in features} for i in range(len(result)): for j in range(len(result[i])): if result[i][j][2] == \u0026quot;Left\u0026quot;: decision_region[features[result[i][j][0]]][0].append(result[i][j][1]) else: decision_region[features[result[i][j][0]]][1].append(result[i][j][1]) decision_region_ = {} for k in features: try: upper_bound = min(decision_region[k][0]) except ValueError: upper_bound = \u0026quot;Unknown\u0026quot; try: lower_bound = max(decision_region[k][1]) except ValueError: lower_bound = \u0026quot;Unknown\u0026quot; decision_region_[k] = [lower_bound, upper_bound] value_to_remove = [\u0026#39;Unknown\u0026#39;, \u0026#39;Unknown\u0026#39;] decision_region_ = {key: value for key, value in decision_region_.items() if value != value_to_remove} value_to_remove = [0.5, 0.5] decision_region_ = {key: value for key, value in decision_region_.items() if value != value_to_remove} return (decision_region_) Test:\nfeatures = X.columns summarize_region(result, features) Outputs:\n{\u0026#39;X_1_blue\u0026#39;: [0.5, \u0026#39;Unknown\u0026#39;], \u0026#39;X_1_red\u0026#39;: [\u0026#39;Unknown\u0026#39;, 0.5], \u0026#39;X_2\u0026#39;: [6.35, 4.2]} From the output above, we know that the decision region:\n{blue} * [6.35, 4.2] But it seems that the region [6.35, 4.2] is not reasonable due to the poorly generated data. But it may happens in some situations, which may require us to come up with new ways to ensemble these terminal nodes.\n ","date":1530921600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530921600,"objectID":"0093d29b100375be265b5486a11eaca1","permalink":"/post/decison-tree-path/decision-tree-how-to-find-the-path-from-the-root-to-the-desired-terminal-node/","publishdate":"2018-07-07T00:00:00Z","relpermalink":"/post/decison-tree-path/decision-tree-how-to-find-the-path-from-the-root-to-the-desired-terminal-node/","section":"post","summary":"Find terminal nodes in each tree of the built random forest that give the largest prediction. Then find paths from root to these selected terminal nodes and ensemble them to derive a decision region.","tags":["RandomForest"],"title":"Decision Tree: How to find the path from the root to the desired terminal node","type":"post"},{"authors":null,"categories":null,"content":"In this project, we\n Develope data products to help Airbnb hosts to determine listing prices using Sparse Regression and RandomForest Researched how amenities and geolocation in uence listing prices Designed a User Interface for customers to gain insight into Airbnb rental markets in Boston  The Video Presentation and Rshiny App Demo are also provided.\n ","date":1512882000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512882000,"objectID":"690845e8dc7a3b2b3563f360194fbc75","permalink":"/project/boston-housing/","publishdate":"2017-12-10T00:00:00-05:00","relpermalink":"/project/boston-housing/","section":"project","summary":"Develope data products to help Airbnb hosts to determine listing prices.","tags":["Data Analysis"],"title":"Real Estate Market Data Analysis","type":"project"},{"authors":null,"categories":null,"content":"This is the deep learning course final project trying to reporduce the results reported in the paper, Show and Tell: A Neural Image Caption Generator.\nThe model is trained on the MS coco2014 dataset. Our final result looks like\nFor code, UI, and report please click here.\n","date":1512882000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512882000,"objectID":"217bfb70762bb4746b8027e79602a247","permalink":"/project/show-and-tell/","publishdate":"2017-12-10T00:00:00-05:00","relpermalink":"/project/show-and-tell/","section":"project","summary":"Feed in an image, AI will generate the caption for you!","tags":["Deep Learning","CNN","LSTM"],"title":"Show and Tell: A Neural Image Caption Generator","type":"project"},{"authors":null,"categories":null,"content":"This is the statistical computing course final project, trying to understand, reporduce and extend some results reported in the paper, Variational Inference: A Review for Statisticians.\nThree datasets are used here, simulated data, old faithful and imageCLEF.\nOur final result looks like\n Simulated data   old faithful  imageclef\n  For code and report please click here.\n","date":1512882000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512882000,"objectID":"ef4fca63f9151ea900f34782ac9bfb47","permalink":"/project/variational-inference/","publishdate":"2017-12-10T00:00:00-05:00","relpermalink":"/project/variational-inference/","section":"project","summary":"Clustering under the variation inference framework.","tags":["Data Analysis"],"title":"Variational Gaussian Mixtures","type":"project"},{"authors":null,"categories":["optimization"],"content":"  Problem description Notations Assumption Algorithm  Convergence Analysis Powell’s example R codes for numerical experiments   We mainly focus on the convergence of Block coordinate decent with exact minimization, whose block update strategy employs Gauss-Seidel manner. And then use Powell’s example to see what will happen if some conditions are not met.\n Reference: 1. Dimitri .P Bertsekas, Nonlinear Programming 2ed 2. Powell ,1973, ON SEARCH DIRECTIONS FOR MINIMIZATION ALGORITHMS\n Problem description Notations We want to solve the problem:\n\\[\\mathop{min}_{x\\in X}\\quad f(x)\\]\nwhere X is a Cartesian product of closed convex sets $X_1,…,X_m:X=_{i=1}^n X_i $\nWe assume that \\(X_i\\) is a closed convex subset of \\(R^{n_i}\\) and \\(n=\\sum_{i=1}^m n_i\\). The vector is partitioned into \\(m\\) block(s) such that \\(x_i \\in X^{n_i}\\).\nWe denote \\(\\nabla_i f\\) as the gradient of \\(f\\) with respect to component \\(x_i\\).\n Assumption We shall assume that for every \\(x\\in X\\) and \\(i=1,2,...m\\) the optimization problem\n\\[\\mathop{min}_{\\xi\\in X_i}\\quad f(x_1,...,x_{i-1},\\xi,x_{i+1,....,x_m})\\]\nhas at least one solution.\n Algorithm The Gauss-Seidel method, generates the next iterate \\(x^{k+1}=(x^{k+1}_1,...,x^{k+1}_m)\\), given the current the iterate \\(x^{k}=(x^{k}_1,...,x^{k}_m)\\), according to the iteration\n\\[x^{k+1}_i=\\mathop{argmin}_{\\xi\\in X_i}\\quad f(x_1^{k+1},...,x^{k+1}_{i-1},\\xi,x^k_{i+1},...,x_m^k)\\]\n  Convergence Analysis Theorem Suppose that \\(f\\) is continuously differentiable over the set \\(X\\) defined as above. Furthermore, suppose that for each \\(i\\) and \\(x\\in X\\),\n\\[f(x_1,...,x_{i-1},\\xi,x_{i+1,....,x_m})\\]\nviewed as a function of \\(\\xi\\), attains a unique minimum \\(\\bar x_i\\) over \\(X_i\\) and is monotonically non-increasing in the interval from \\(x_i\\) to \\(\\bar \\xi\\). Let \\(\\{x_k\\}\\) be the sequence generated by the block coordinate method with Gauss-Seidel manner. Then, every limit point of \\(\\{x_k\\}\\) is a stationary point.\nPROOF\nLet\n\\[z_i^k=(x_1^{k+1},...,x_i^{k+1},x_{i+1}^k,...,x_m^k)\\]\nBy the nature of this algorithm, for all \\(k\\geq 0\\), we have following inequality\n\\[f(x^k)\\geq f(z_1^k)\\geq f(z_2^k)\\geq ...\\geq f(z_{m-1}^k)\\geq f(x^{k+1}) \\quad (*)\\]\nSince \\(\\{x_k\\}in X\\), we can assume \\(\\{x^{k_j}\\}\\) is the subsequence that converges to \\(\\bar x=(\\bar x_1,..,\\bar x_m)\\).\nNow we want prove that \\(\\bar x\\) is the stationary point of \\(f\\).\nFrom (*), we know that\n\\[f(z_1^{k_j})\\leq f(x_1,x_2^{k_j},..., x_m^{k_j})\\qquad \\forall x_1\\in X_1\\]\nLet \\(j\\rightarrow +\\infty\\), we derive\n\\[f(\\bar x)\\leq f(x_1,\\bar x_2,..., \\bar x_m)\\overset \\Delta = h(x_1)\\qquad \\forall x_1\\in X_1\\]\nwhich implies that \\(\\bar x_i\\) is the minima of \\(h(x_1)\\) on \\(X_1\\). Using the optimality over a convex set, we conclude that\n\\[h\u0026#39;(\\bar x_1)(\\bar x_1 -x_1)\\geq 0 \\Leftrightarrow (x_1-\\bar x_1)^T\\nabla_1f(\\bar x_1)\\geq 0\\qquad x_1\\in X_1\\]\nAt this stage, if we can prove that \\(\\{z_1^{k_j}\\}\\) converges to \\(\\bar x\\), we can show that\n\\[ (x_2-\\bar x_2)^T\\nabla_2 f(\\bar x_2)\\geq 0\\qquad x_2\\in X_2\\], since\n\\[f(z_1^{k_j})=f(x_1^{k_j+1},x_2^{k_j},x_3^{k_j},...,x_m^{k_j})\\leq f(x_1^{k_j+1},x_2,x_3^{k_j},...,x_m^{k_j})\\qquad x_2\\in X_2\\]\nLet \\(j\\rightarrow +\\infty\\), we derive\n\\[f(\\bar x)\\leq f(\\bar x_1,\\bar x_2,\\bar x_3,..., \\bar x_m)\\qquad \\forall x_2\\in X_2\\]\nand\n\\[(x_2-\\bar x_2)^T\\nabla_2f(\\bar x_2)\\geq 0\\qquad x_2\\in X_2\\]\n(Note: Although \\(x_1^{k_j+1}\\) may not in the sequence \\(\\{x_1^{k_t}\\}_{t\\geq 1}\\) ,which convergences to \\(\\bar x_1\\), but \\(\\{z_1^{k_j}\\}\\) converges to \\(\\bar x\\), so its component \\(x_1^{k_j+1}\\) converges to \\(\\bar x_1\\)).\nFurthermore, if we prove that for \\(i=1,2,...,m-1\\),\\(\\{z_i^{k_j}\\}\\) convergences to \\(\\bar x\\), then we have\n\\[(x_i-\\bar x_i)^T\\nabla_i\\;f(\\bar x_i)\\geq 0\\qquad x_i\\in X_i\\]\nAnd thus \\(\\bar x\\) is a stationary point, since \\((x-\\bar x)^T\\nabla f(\\bar x)\\geq 0\\)\nBy far, it remains to prove that \\(\\{z_i^{k_j}\\}\\quad,\\forall i\\) convergence to \\(\\bar x\\). First,we try to prove that \\(\\{z_1^{k_1}\\}\\) convergence to \\(\\bar x\\).\nAssume the contrary that \\(r^{k_j}=\\vert \\vert z_1^{k_j}-x^{k_j}\\vert \\vert\\) doesn’t convergence to 0. Let \\(s_1^{k_j}=(z_1^{k_j}-x^{k_j})/r^{k_j}\\). Thus, \\(z_1^{k_j}=x^{k_j}+r^{k_j}s_1^{k_j}\\) , \\(\\vert \\vert r_{k_j}\\vert \\vert =1\\) and \\(s_1^{k_j}\\) differs from 0 only along the first block-component. Since \\(\\{s_1^{k_j}\\}\\) belong to a compact set and therefore without loss of generality, we assume \\(s_1^{k_j}\\) convergences to \\(\\bar s_1\\).\nSince \\(r^{k_j}\u0026gt;0\\),we can find a \\(\\epsilon\\in (0,1)\\), such that \\(x^{k_j}+\\epsilon s_1^{k_j}\\) lies on the segment joining \\(x^{k_j}\\) and \\(x^{k_j}+s_1^{k_j}=z_1^{k_j}\\). Using the non-increasing property of \\(f\\),we derive,\n\\[f(z_1^{k_j})\\leq f(x^{k_j}+\\epsilon s_1^{k_j}) \\leq f(x^{k_j})\\]\nAgain, using (*), we conclude\n\\[f(x^{k_{j+1}})\\leq f(z_1^{k_j})\\leq f(x^{k_j}+\\epsilon s_1^{k_j}) \\leq f(x^{k_j})\\]\nLet \\(j\\rightarrow +\\infty\\), we derive \\(f(\\bar x)=f(\\bar x+\\epsilon \\bar s_1)\\), which contradicts the hypothesis that \\(f\\) is uniquely minimized when viewed as a function of the first block component. This contradiction establishes that \\(\\{z_1^{k_1}\\}\\) convergence to \\(\\bar x\\).\nSimilarly, let \\(r_t^{k_j}=\\vert \\vert z_t^{k_j}-z_{t-1}^{k_j}\\vert \\vert\\) for \\(t=2,3,...,m-1\\) and using the same technique shown above, we finally prove that \\(\\{z_i^{k_j}\\},\\quad \\forall i\\).\n Powell’s example In ON SEARCH DIRECTIONS FOR MINIMIZATION ALGORITHMS, Power actually gives three examples that sequences generated by the algorithm discussed above do not convergence to stationary points once some hypothesis are not met.\n The first example is straightforward, However, the remarkable properties of this example can be destroyed by making a small perturbation to the starting vector \\(x^0\\).\n The second example is not sensitive to either small changes in the initial data or to small errors introduced during the iterative process, for example computer rounding errors.\n The third example suggests that a function that is infinitely differentiable that also causes an endless loop in the iterative minimization method.\n  We here only presents the first example. Consider the following function\n\\[f(x,y,z)=-(xy+yz+zx)+(x-1)_+^2+(-x-1)_+^2+(y-1)_+^2+(-y-1)_+^2+(z-1)_+^2+(-z-1)_+^2\\]\nwhere\n\\[(x-c)_+^2=\\begin{cases}0,x-c\u0026lt; 0\\\\ (x-c)^2,x-c\\geq 0\\end{cases}\\]\nGiven the starting point \\(x_0=(-1-e,1+\\frac{1}{2}e,-1-\\frac{1}{4}e)\\) and use block coordinate decent algorithm,and we update the variable in a manner of \\(x\\rightarrow y\\rightarrow z\\rightarrow x ...\\) with\n\\[x_{k+1}^{**}\\leftarrow \\text{sign}(y_k+z_k)[1+\\frac{1}{2}\\vert y_k+z_k\\vert ]\\]\n\\[y_{k+1}^{**}\\leftarrow \\text{sign}(x_{k+1}+z_k)[1+\\frac{1}{2}\\vert x_{k+1}+z_k\\vert ]\\]\n\\[z_{k+1}^{**}\\leftarrow \\text{sign}(x_{k+1}+y_{k+1})[1+\\frac{1}{2}\\vert x_{k+1}+y_{k+1}\\vert ]\\]\nWe here present the first six steps of this case\n  cycle/totall iteration x y z    1/1 1+\\(\\frac{1}{8}e\\) 1+$e $ -1-\\(\\frac{1}{4}e\\)  1/2 1+\\(\\frac{1}{8}e\\) -1-\\(\\frac{1}{16}e\\) -1-\\(\\frac{1}{4}e\\)  1/3 1+\\(\\frac{1}{8}e\\) -1-\\(\\frac{1}{16}e\\) 1+\\(\\frac{1}{32}e\\)  2/4 -1-\\(\\frac{1}{64}e\\) -1-\\(\\frac{1}{16}e\\) 1+\\(\\frac{1}{32}e\\)  2/5 -1-\\(\\frac{1}{64}e\\) 1+\\(\\frac{1}{128}e\\) 1+\\(\\frac{1}{32}e\\)  2/6 -1-\\(\\frac{1}{64}e\\) 1+\\(\\frac{1}{128}e\\) -1-\\(\\frac{1}{256}e\\)  3/7 1+\\(\\frac{1}{512}e\\) 1+\\(\\frac{1}{128}e\\) -1-\\(\\frac{1}{256}e\\)  … … … …    This result implies that the sequence obtained by this algorithm can not converge to one single point since \\(x-coordinate\\) change its sign as the even cycle and odd cycle alternate. Situations are similar for \\(y-coordinate\\) and \\(z-coordinate\\).\nBut \\(\\{x_k\\}\\) has six sub-sequences which convergence to (1,1,-1), (1,-1,-1), (1,-1,1), (-1,-1,1),(-1,-1,1),(-1,1,1),(-1,1,-1) respectively.\n Remark\nA hint to derive the update formula:\n\\[x\\leftarrow \\text{sign}(y+z)[1+\\frac{1}{2}(y+z)]\\]\nIndeed, derivates of \\((x-1)_+^2\\) and \\((-x-1)_+^2\\) are as follows respecively\n\\[\\frac{d(x-1)_+^2}{dx}=\\begin{cases}2(x-1),x\\geq 1\\\\0,x\u0026lt;1\\end{cases}\\quad \\frac{d(-x-1)_+^2}{dx}=\\begin{cases}2(-x-1),x\\leq -1\\\\0,x\u0026gt;-1\\end{cases} \\]\nSo for the univariate optimization problem, setting the derivate of \\(g(x)=f(x,y,z)\\) to zero, we conclude\n\\[\\frac{\\partial f(x,y,x)}{\\partial x}=0\\Rightarrow \\begin{cases}x\\geq 1: x=1+\\frac{1}{2}(y+z)\\\\-1\u0026lt; x\u0026lt;1: -(y+z)=0\\\\x\\leq -1:x=-1+\\frac{1}{2}(y+z) \\end{cases}\\]\n The gradient of \\(f(x,y,z)\\) on this cyclic path, is \\(\\nabla f(x,y,z)=(-y-z,-x-z,-x-y)\\) and \\(\\vert \\vert \\nabla f(x,y,z)\\vert \\vert _1=2\\)\n This example is unstable with respect to small perturbations. Small changes in the starting point \\(x_0=(-1-e,1+\\frac{1}{2}e,-1-\\frac{1}{4}e)\\) or smal errors in the numbers that are computed during the calculation will destroy the cyclic behavior.\nIt’s s clear the choice of perturbations \\(e\\) plays a key role. Say, \\(x_0=(-1-e_1,1+e_2,-1-e_3)\\) and we have \\(e_k=\\frac{1}{2}(e_{k-2}- e_{k-1})\\)\n  cycle/totall iteration x y z    1/1 1+\\(e_4\\) 1+\\(e_2\\) -1-\\(e_3\\)  1/2 1+\\(e_4\\) -1-\\(e_5\\) -1-\\(e_3\\)  1/3 1+\\(e_4\\) -1-\\(e_5\\) 1+\\(e_6\\)  2/4 -1-\\(e_7\\) -1-\\(e_5\\) 1+\\(e_6\\)  2/5 -1-\\(e_7\\) 1+\\(e_8\\) 1+\\(e_6\\)  2/6 -1-\\(e_7\\) 1+\\(e_8\\) -1-\\(e_9\\)  … … … …    To preserve the cyclic behavior , we have to make sure that \\(e_{k-2}\u0026gt;e_{k-1}\\)\nAnd in practice, when we do some numerical tests, we shall find that, this theoretically-existed endless loop actual breaks down due to the rounding errors. A brief illustration is given below. In this experiment, loop ends at the 52 steps.\n As\n\\[\\frac{\\partial f(x,y,x)}{\\partial x}=0\\Rightarrow \\begin{cases}x\\geq 1: x=1+\\frac{1}{2}(y+z)\\\\-1\u0026lt; x\u0026lt;1: -(y+z)=0\\\\x\\leq -1:x=-1+\\frac{1}{2}(y+z) \\end{cases}\\]\nsuggests that, when \\(-1\u0026lt;x\u0026lt;1\\), the choice of \\(x\\) is arbitrary and we set \\(x^*=0\\) in the case above. So the uniqueness requirement is violated. It turns out that the six vertices are even not the stationary points.\nFor example, at point \\(\\bar x=(1,1,-1)\\), \\(\\nabla f(\\bar x)=(0,0,-2)\\) and for any ponit \\(x\\) in the unit cubic \\((x-\\bar x)^T\\nabla f(\\bar x)\\leq 0\\). Say, \\(x=(0.9,0.9,-0.9)\\), \\((x-\\bar x)^T\\nabla f(\\bar x)=-0.2\u0026lt;0\\)\nActually, as in the proof of Theorem, we prove that \\(\\{z_1^{k_j}\\}\\) converges to \\(\\bar x\\), where \\(\\bar x\\) is the limit point of \\(\\{x^{k_j}\\}\\). But in this example, the limit point of \\(\\{z_1^{k_j}\\}\\) is (1,1,-1) while the limit point of \\(\\{x^{k_j}\\}\\) is either (-1,1,-1) or (1,-1,1). So the requirement of uniqueness is not met.\n   R codes for numerical experiments #################### ### Function for test ### #################### PowellE1\u0026lt;-function(xstart,cycles,fig=T){ #######function part ############## UpdateCycle\u0026lt;-function(x){ Sign\u0026lt;-function(x){ if (x\u0026gt;0){ return(1) }else{ if (x\u0026lt;0){ return(-1) }else{ return(0) } } } x.new\u0026lt;-c() x.new[1]\u0026lt;-Sign(x[2]+x[3])*(1+0.5*abs(x[2]+x[3])) x.new[2]\u0026lt;-Sign(x.new[1]+x[3])*(1+0.5*abs(x.new[1]+x[3])) x.new[3]\u0026lt;-Sign(x.new[1]+x.new[2])*(1+0.5*abs(x.new[1]+x.new[2])) cycle\u0026lt;-matrix(c(x.new[1],x[2],x[3],x.new[1],x.new[2],x[3],x.new[1],x.new[2],x.new[3]), ncol=3,byrow=T) return(cycle) } fpowell\u0026lt;-function(x){ PostivePart\u0026lt;-function(x){ ifelse(x\u0026gt;=0,x,0) } fval\u0026lt;-(-(x[1]*x[2]+x[2]*x[3]+x[1]*x[3]))+ PostivePart(x[1]-1)^2+PostivePart(-x[1]-1)^2+ PostivePart(x[2]-1)^2+PostivePart(-x[2]-1)^2+ PostivePart(x[3]-1)^2+PostivePart(-x[3]-1)^2 return(fval) } ############ operation part ################ x.store\u0026lt;-matrix(ncol=3,nrow=cycles*3+1) x.store[1,]\u0026lt;-xstart for (i in seq_len(cycles)){ x.store[(3*i-1):(3*i+1),]\u0026lt;-UpdateCycle(x.store[3*i-2,]) } x.store\u0026lt;-x.store[-1,] fval\u0026lt;-rep(0,cycles*3) for(i in seq_len(cycles*3)){ fval[i]\u0026lt;-fpowell(x.store[i,]) } fval\u0026lt;-as.matrix(fval) if (fig==T){ plot(fval,ylim=c(min(fval)-1,max(fval)+1),type=\u0026quot;l\u0026quot;,xlab=\u0026quot;Iterations\u0026quot;,ylab = \u0026quot;F value\u0026quot;) } r\u0026lt;-list() r$x.iterate\u0026lt;-x.store r$fval\u0026lt;-fval return(r) } ################## #### Test 1 ######## ################## perturb\u0026lt;-0.5 xstart\u0026lt;-c(-1-perturb,1+0.5*perturb,-1-0.25*perturb) cycles\u0026lt;-20 r\u0026lt;-PowellE1(xstart,cycles,fig=T) ################## #### Test 2 ######## ################## perturb\u0026lt;-0.5 xstart\u0026lt;-c(-1-perturb,1+0.5*perturb,-1-0.25*perturb) cycles\u0026lt;-20 r\u0026lt;-PowellE1(xstart,cycles,fig=T) ################## #### Test 3 ######## ################## xstart\u0026lt;-c(3,2,1) cycles\u0026lt;-100 r\u0026lt;-PowellE1(xstart,cycles,fig=T)  ","date":1479340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479340800,"objectID":"8412461298db1d9651b4bc3fba2d8fb8","permalink":"/post/powell/convergence-analysis-for-block-coordinate-descent-algorithm-and-powells-examples/","publishdate":"2016-11-17T00:00:00Z","relpermalink":"/post/powell/convergence-analysis-for-block-coordinate-descent-algorithm-and-powells-examples/","section":"post","summary":"Convergence analysis of Block coordinate decent with exact minimization.","tags":["Powell-Example"],"title":"Convergence Analysis for Block Coordinate Decent Algorithm and Powell's Examples","type":"post"}]