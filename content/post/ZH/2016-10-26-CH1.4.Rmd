---
title: "无约束优化(4)"
author: Yutong Dai
date: '2016-10-26'
categories:
  - convex optimization
tags:
  - 无约束优化
slug: uncon4
output:
  blogdown::html_page:
    toc: yes
summary: 该系列posts是笔者学习Dimitri .P Bertsekas所写的 Nonlinear Programming 2ed 中文版时整理的笔记。第一章无约束优化内容涉及到最优性条件、梯度方法、牛顿法与变形以及共轭方向法。本文讨论牛顿法与其变形。
---


## 牛顿法与其变形

之前我们讨论了梯度方法的收敛性与收敛速率。我们知道最速下降法是梯度方法中迭代形式最简单，但是收敛速率最慢的算法。另一个极端是牛顿法，它在一定情况下它是梯度算法中收敛速率最快的算法，但是相应的迭代算法会比较复杂，因为要计算Hessian阵的逆。

如前面所讨论的牛顿法方向的选取方式

$$d_k=-(\nabla^2 f(x_k))^{-1}\nabla f(x_k)$$

保证了其尺度变化不变性(scaling-free)。 即:
考虑线性变化$x=Sy$，则在变量y空间中用牛顿法极小化$f(S^{-1}x)$生成的序列$y_{k+1}=y_k+a_kd_k$与在变量x空间用牛顿法极小化$f(x)$生成的序列$x_{k+1}=x_k+a_kd_k$存在关系$x_k=Sy_k$。这种尺度变化不变性是最速下降法所不具备的。

**纯牛顿法**

由【类牛顿法的超线性收敛速率结论】我们知道，在非奇异的局部最小值点附近，当Hessian阵正定时，只需选取步长s=1。因此即将收敛时，算法的跌带形式为

$$x_{k+1}=x_{k}-(\nabla^2f(x_k))^{-1}\nabla f(x_k)$$

该算法称为纯牛顿法。

**Remark**

 在远离局部最小值时，有可能出现

* Hessian阵奇异
* Hessian阵非正定导致$d_k$不是下降方向

因此，对于牛顿法我们要进行下面的两个讨论

* 局部收敛性：刻画标准牛顿方法在非奇异局部最小值附近的性能
* 全局收敛性： 当起始点离所有局部最小值点都很远的时候，为了保证算法收敛且能够达到局部最小值点，需要对牛顿法进行变形。 


###局部收敛性

鉴于我们已经讨论过纯牛顿法的收敛性，这里我们就提出更为一般的牛顿法的收敛性结论。

考虑含有n个未知数，n个方程的约束系统

$$g(x)=0$$

其中$g:R^n\vert \rightarrow R^n$ 是一个连续可微的函数。
求解该问题的迭代算法为 

$$x_{k+1}=x_k-(\nabla g(x)_k)^{-1}g(x_k))$$

**Remark**

1. 其实该问题可以看做n元方程组的求解问题。因此我们可以得到更一般的Newton-Raphson算法。该迭代式的导出的直观想法与梯度方法十分类似

$$f(x)\approx f(x_0)+(x-x_0)^T\nabla f(x_0)$$

于是求解$g(x)=0$近似与求解$f(x_0)+(x-x_0)^T\nabla f(x_0)=0$，进而得到迭代关系式。
2. 若$\nabla f(x)=g(x)$则上述牛顿法就是我们所说的纯牛顿法。但是一般来说可微的向量值函数$g$不一定是某个函数的梯度。特别的，$g$是某个函数$f$的梯度当且仅当对所有的x，$\nabla g(x)$是对称矩阵。
3. 该算法是超线性收敛的。【证明可以参见Dimitri .P Bertsekas, Nonlinear Programming 2ed中文版p68】
4. 更一般的结论
	![](http://ogfa13jyv.bkt.clouddn.com/14769767590504.jpg)
	![](http://ogfa13jyv.bkt.clouddn.com/14769767820494.jpg)

5. 该算法的问题在于，我们不能够知道迭代初始点是否离局部最小值点足够近。事实上当初始点比较坏时，算法完全可能不收敛。
![Snip20161020_34](http://ogfa13jyv.bkt.clouddn.com/Snip20161020_34.png)


### 全局收敛性

正如我们之前所讨论的，纯牛顿法在解$f$无约束的最小化问题会遇到以下问题：
 
 * $(\nabla^2 f(x_k))^{-1}$的逆不存在。比如$f$是线性函数，当$f(x)=C'x$时$\nabla^2 f(x_k)=0$
 * 纯牛顿法不能保证函数在每一步下降，即出现$f(x_{k+1})f(x_k)$的情形
 * 从更一般的牛顿法的迭代形式可以看出，我们纯牛顿法只是在试图解决$\nabla f(x)=0$的问题，因此算法可能会在局部最大值点附近。

 于是我们需要对牛顿法作出一些修正。比如给Hessian阵$\nabla^2 f(x_k)$加一个对角阵扰动的扰动$\nabla^2 f(x_k)+\Omega_k$使得其正定从而可逆。比如改进的Cholesky分解法，于是衍生出了牛顿法的变形

### Gauss-Newtown 法

考虑极小化$\frac{1}{2}\vert \vert g(x)\vert \vert ^2$，其中$g: R^n\vert \rightarrow R^m$的专门方法：Gauss-Newtown法。

 * 考虑$g(x)$的线性逼近
 	
 	$$\tilde{g}(x,x_k)=g(x_k)+(x-x_k)'\nabla g(x_k)$$
 
 * 极小化$\tilde{g}(x,x_k)$得到$x_{k+1}$，即
 	
 	$$x_{k+1}=\mathop{argmin}_{x}\frac{1}{2}\vert \vert \tilde{g}(x,x_k)\vert \vert ^2\overset{*}=x_{k}-\big(\nabla g(x_k)\nabla^T g(x_k)\big)^{-1}\nabla g(x_k)g(x_k)$$

**Remark**

1. 检验 $\overset{*}=$ 成立，只用对$\frac{1}{2}\vert \vert \tilde{g}(x,x_k)\vert \vert ^2$关于$x$求导，并设为零即可。
2. 当$g$是一个线性函数的时候，比如线性回归模型$g(\beta)=Y-X\beta$，迭代一步即可收敛。因为$\tilde{g}(x,x_k)$不再是$g(x)$的线性逼近，而就是$g(x)$的恒等变换。关于$x$极小化$\tilde{g}(x,x_k)$等价于原问题。

#### 算法收敛性

下降方向
	
$$d_k=-\big(\nabla g(x_k)\nabla^T g(x_k)\big)^{-1}\nabla g(x_k)g(x_k)$$

在$\nabla g(x_k)\nabla^T g(x_k)$奇异时，算法会出现问题。为了解决这个问题，同时提高算法在奇异点附近的迭代收敛性，我们一般会修正下降方向为
	
$$d_k=-\big(\nabla g(x_k)\nabla^T g(x_k)+\Delta_k\big)^{-1}\nabla g(x_k)g(x_k)$$

其中$\Delta_k$为对角阵，且正定（Levenberg-Marquatdt提出$\Delta_k=\lambda I,\lambda$为一正数）并根据一定准则选择下降步长$a_k$，从而有更新公式
	
$$x_{k+1}=x_{k}-a_k\big(\nabla g(x_k)\nabla^T g(x_k)+\Delta_k\big)^{-1}\nabla g(x_k)g(x_k)$$

通过这些$\Delta_k$的选取方式，可以保证$$\{d_k\}$$是梯度相关的，比如我们再用Armijo准则选择合适的步长，则算法一定是收敛的（因为此时可以看成是特殊的梯度法）。

#### 与牛顿法的关系

两种算法十分接近，Gauss-Newtown法本质上是牺牲收敛速度来避免复杂的二阶项计算。事实上，考虑$g$的每个分量$g_i$是标量的情形，则目标函数$\frac{1}{2}\vert \vert g(x)\vert \vert ^2$的Hessian阵为

$$\nabla g(x_k)\nabla^T g(x_k)+\sum_{i=1}^m \nabla^2 g_i(x_k)g_i(x_k)$$

于是牛顿法的更新公式为

$$x_{k+1}=x_{k}-\big(\nabla g(x_k)\nabla^T g(x_k)+\sum_{i=1}^m \nabla^2 g_i(x_k)g_i(x_k)\big)^{-1}\nabla g(x_k)g(x_k)$$

而Gauss-Newtown法的更新公式为

$$x_{k+1}=x_{k}-\big(\nabla g(x_k)\nabla^T g(x_k)\big)^{-1}\nabla g(x_k)g(x_k)$$

如果被忽略的二阶项在收敛点附近值很小，自然Gauss-Newtown法的速率是令人满意的。