---
title: "无约束优化(2)"
author: Yutong Dai
date: '2016-09-28'
categories:
  - convex optimization
tags:
  - 无约束优化
slug: uncon2
output:
  blogdown::html_page:
    toc: yes
summary: 该系列posts是笔者学习Dimitri .P Bertsekas所写的 Nonlinear Programming 2ed 中文版时整理的笔记。第一章无约束优化内容涉及到最优性条件、梯度方法、牛顿法与变形以及共轭方向法。本文讨论梯度方法中下降方向和步长的选择。
---

## 梯度方法

$$outline\begin{cases}
下降方向选择\begin{cases}
			最速下降\\
			牛顿法\\
		    \end{cases}\\\\
步长的选择\begin{cases}
			固定步长准则\\
			缩减步长准则\\
			最小化准则\\
			Armijo准则
		    \end{cases}\\\\
收敛性分析\begin{cases}
			极限点的存在性问题\\
			迭代终止条件\\
			收敛性结论
		    \end{cases}\\
收敛速率分析
\end{cases}$$

### 直观分析

### 下降方向



#### 最速下降
#### 牛顿法
修正的牛顿法


### 步长准则
#### 最小化准则
#### Armijo准则

>设$f$是连续可微函数, $\sigma,\beta,s$是固定的标量，其中$0<\beta<1$，$0<\sigma<1$,令$\alpha_k=\beta^{m_k}s$，其中$m_k$是满足下式的第一个非负整数m
$$f(x_{k+1})\leq f(x_k)+\sigma\alpha_kd_k^T\nabla f(x_k)$$

**Remark**

1. 准则的直观解释。考虑f(x_{k+1})在$x_k$处的一阶Taylor 展开
$$f(x_{k}+\alpha_kd_k)=f(x_k)+\alpha_kd_k^T\nabla f(x_k)+o(\alpha_k)$$
由下降条件知道$d_k^T\nabla f(x_k)<0$而当$\alpha_k\rightarrow 0$时总是存在$\alpha_k$使得$f(x_{k}+\alpha_kd_k)\leq f(x_k)+\alpha_kd_k^T\nabla f(x_k)$成立
2. $\sigma$保证满足准则的$\alpha_k$可以更快的找到($\alpha_k$是从递减序列$\{\beta^ms\}_{m=1}^{+\infty}$中寻找的).因此通常$\sigma\in[10^{-5},10^{-1}]$
3. $\beta$是收缩因子保证步长初始步长s可以在必要时刻减小

**Expansion**

$F_c(x)=f(x)+cp(x)$其中$c>0$，$f(x)$是连续可微函数，$p(x)$是非光滑的惩罚项。我们想要极小化$F_c(x)$，那么我们可以用梯度方法进行求解。具体来说

1. 寻找下降方法$d_k$，其中$H_k$是$\nabla^2 f(x_k)$的正定近似
$$d_k=\mathop{argmin}_d \nabla f(x_k)^Td+\frac{1}{2}d^TH_kd+cp(x_k+d)$$
2. Armijo rule寻找步长$\alpha_k$

> $0<\beta<1$，$0<\sigma<1$,$0\leq\gamma<1$令$\alpha_k=\beta^{m_k}s$，其中$m_k$是满足下式的第一个非负整数m
$$F_c(x_k+\alpha_kd_k)\leq F_c(x_k)+\sigma\alpha_k\Delta_k$$
其中$\Delta_k=d_k^T\nabla f(x_k)+\gamma d_k^TH_kd_K+cp(x_k+d_k)-cp(x_k)$

**Remark**: Paul Tseng et al(2007)证明了在一定条件下$\Delta_k<0$

#### 固定步长准则
#### 缩减步长准则