---
title: Explore the Linear Mixed Effect Model
author: Yutong Dai
date: '2018-03-16'
lastmod: '2018-06-06'
slug: explore-the-linear-mixed-effect-model
categories:
  - Statistics
tags:
  - LME
output:
  blogdown::html_page:
    toc: yes
summary: "Basic concepts and R commands to play with LME."
header-includes:
  - \newcommand{\iid}{\overset{iid}\sim}
  - \usepackage{bm}
  - \newcommand{\bX}{\bm{X}}
---

> References: Pinheiro, J. C., & Bates, D. (2009). Mixed-Effects Models in S and S-PLUS. *Statistics and Computing*, Springer. 

```{r, echo=FALSE}
suppressWarnings(suppressMessages(library(dplyr))) 
plot.experiment <- function(response, experimental, blocking, lab.x=NULL, lab.y=NULL, lab.experimental=NULL, jitter=FALSE){
  suppressMessages(require(ggplot2)) 
  df <- data.frame(response,experimental,blocking)
  if (jitter) {
    p <- ggplot(df, aes(x=response, y=blocking, colour=experimental, shape=experimental)) + 
      geom_point(position = position_jitter(w = diff(range(response)) * 0.05, h = 0))}else{
        p <- ggplot(df, aes(x=response, y=blocking, colour=experimental, shape=experimental)) +
          geom_point()
      }
  if (!is.null(lab.x)) {p <- p + xlab(lab.x)}
  if (!is.null(lab.y)) {p <- p + ylab(lab.y)}
  if (is.null(lab.experimental)) {
    p <- p + guides(colour=guide_legend(title=NULL), shape=guide_legend(title=NULL)) 
  }else{
   p <- p + guides(colour=guide_legend(title=lab.experimental), shape=guide_legend(title=lab.experimental))
  }
  p + theme(legend.position="top")
}
```
# Why Ramdom Effect Models

Compared with the linear fixed effect models, the advantages of modeling data with linear mixed-effect models (MLE) are:

1. Rather than models the specific type of sample of *levels* from the **factor** of interest, e.g *Rail tyep I* and *Rail type II* sampled from all possible types of the **Rail**, LME models the factor from the population perspective.

2. Provide an estimate of the between-level variability.

3. Prevent the model complexity (\# of parameters) increases linearly with the increase with \# of levels.

# One-way classification with `Rail` example

## `Rail` Data
```{r}
suppressMessages(library(nlme))
suppressMessages(library(kableExtra))
knitr::kable(t(Rail)) %>%  kable_styling("striped", "hover") %>%
  scroll_box(width = "100%", height = "100%")
```

```{r}
plot(Rail)
```

## Modeling

**Model and assumptions** 
$$
y_{ij} = \mu + b_i + \epsilon_{ij} 
$$

* $b_i \sim N(0,\sigma_b^2)$;

* $\epsilon_{ij} \sim N(0,\sigma^2)$.

| parameters | explanation  |
| --- | --- |
| $\mu$ | population mean |
|$\sigma_b^2$|between-rail variability|
|$\epsilon_{ij}$|noise|
|$\sigma^2$|within-rail variability|

1. $b_i$ represents the deviation of the i-th type of rail from the population, it is not a parameter.
2. Note **this is a balcance design.**

## Fitting the model with the package `nlme`

```{r}
rail_data <- Rail
lm.random <- lme(fixed = travel~1, data=rail_data, random = ~1 | Rail)
# fixed = travel~1: only the intercept is the fixed effect
# random = ~1 | Rail: there is a single random effect for each group and the grouping is given by the Rail
summary(lm.random)
```



* $\hat\mu =$ `r lm.random$coefficients$fixed[1]`
* $\hat\sigma_b^2=24.80547$
* $\hat\sigma^2=$ `r lm.random$sigma`

## Diagnostic 

**Confidence Intervals on parameters**:

```{r}
intervals(lm.random)
```
The estimation of $\mu,\sigma_b,\sigma$ are really raw and imprecise.

**ANOVA of the fixed effect:**
```{r}
anova(lm.random)
```

# Two-way classification: A Randomized Block Design

For block design, we have two types of factors, one is called *experimental* factor for fixed effects while the other is called *blocking* factor for the random effects.

## `egroStool`: No Replications

We want to make inferences subjects and types of stools are blocking factors.

```{r}
knitr::kable(t(ergoStool)) %>% kable_styling("striped", "hover") %>%
  scroll_box(width = "100%", height = "100%")
```

```{r}
plot(ergoStool)
```


### Modeling

**Model and assumptions:**
$$
y_{ij} = \beta_j + b_i + \epsilon_{ij} 
$$

* $b_i \sim N(0,\sigma_b^2)$;

* $\epsilon_{ij} \sim N(0,\sigma^2)$.

| parameters | explanation  |
| --- | --- |
| $\beta_j$| experimental factor (researchers are interested in)|
|$\sigma_b^2$|between-rail variability|
|$\epsilon_{ij}$|noise|
|$\sigma^2$|within-rail variability|

* Note **this is a balcance design**

#### Choose the contrasts
```{r}
ergoStool %>% filter(Subject == "1") %>% model.matrix(effort ~ Type, .)
```

```{r}
lmod.random <- lme(effort ~ Type, data = ergoStool, random = ~1 | Subject)
summary(lmod.random)
```

* $\hat\beta_1=$intercept: reference level(TypeT1);
* $\hat\beta_i=$ (TypeTi) + intercept i=2,3,4

**Significance Tests for fixed effects** $\beta_2=\beta_3=\beta_4=0$

```{r}
anova(lmod.random)
```

$P(F_{3,24} > 22.3556)<.0001$, there are significance variation within the each type of stools.

### Diagnostic

```{r}
intervals(lmod.random)
```

```{r}
plot(lmod.random,
     form = resid(., type="p") ~ fitted(.) | Subject,
     abline = 0)
```   


## `Machines`: Replications

**With replications, we are able to access the interactions between the experimental and blocking effects!!**

```{r}
knitr::kable(t(Machines)) %>% kable_styling("striped", "hover") %>%
  scroll_box(width = "100%", height = "100%")
```

```{r}
Machines %>% {plot.experiment(.$score,.$Machine,.$Worker,
                lab.x = "Score", lab.y = "Worker", lab.experimental = "Machine")}
```

### Modeling

**Model and assumptions - without interactions:**

$$
y_{ijk} = \beta_j + b_i + \epsilon_{ijk} 
$$

* $b_i \sim N(0,\sigma_b^2)$;

* $\epsilon_{ijk} \sim N(0,\sigma^2)$.

| parameters | explanation  |
| --- | --- |
| $\beta_j$| experimental factor (researchers are interested in)|
|$\sigma_b^2$|between-rail variability|
|$\epsilon_{ijk}$|noise|
|$\sigma^2$|within-rail variability|

* Note **this is a balcance design**

```{r}
lmod.random <- lme(score ~ Machine, data = Machines, random = ~1 | Worker)
summary(lmod.random)
```

**Model and assumptions - with interactions**
$$
y_{ijk} = \beta_j + b_i + b_{ij} + \epsilon_{ijk} 
$$

* $b_i \sim N(0,\sigma_1^2)$;

* $b_{ij} \sim N(0,\sigma_2^2)$; 

* $\epsilon_{ijk} \sim N(0,\sigma^2)$.

| parameters | explanation  |
| --- | --- |
| $\beta_j$| experimental factor (researchers are interested in)|
|$b_i$|blocking factor (source of variablity that is not of the primary interest)|
|$b_{ij}$ |interaction effects between experimental factor and blocking factor|
|$\sigma_b^2$|between-rail variability|
|$\epsilon_{ijk}$|noise|
|$\sigma^2$|within-rail variability|

* Note **this is a balcance design.**

```{r}
# Worker/Machine: Modeling two levels of random effects, Worker and Machine within Worker
lmod.random.interact <- lme(score ~ Machine, data = Machines, random = ~1 | Worker/Machine)
summary(lmod.random.interact)
```

$\beta_1 =52.36$, $\beta_2 = 52.36 + 7.97$, ...
$\sigma_1 = 4.78105$, $\sigma_2 =3.729532$, $\sigma = 0.9615771$.

### Model Selection

$$H_0: \text{the restricted model} \qquad H_A: \text{the general model}$$

```{r}
anova(lmod.random,lmod.random.interact)
```

$LR=2[\log(L_A)-\log(L_0)] \sim \chi^2_{df_A -df_0}$

The likelihood ratio test suggests that model with interatcions are preferred.

## Extensions for interaction terms

```{r}
Machines %>% filter(Worker == "1") %>% {plot.experiment(.$score,.$Machine,.$Worker,
                lab.x = "Score", lab.y = "Worker", lab.experimental = "Machine")}
```

For each worker, say Worker 1, we previously assume

$$
y_{1jk} = \beta_j + b_1 + b_{1j} + \epsilon_{1jk} 
$$


*  $b_1 \sim N(0,\sigma_1^2)$;

*  $b_{1j} \sim N(0,\sigma_2^2)$;

* $\epsilon_{1jk} \sim N(0,\sigma^2)$.

We assume $b_{1j}$ iid $N(0,\sigma_2^2)$, but from the graph above, it seems that it's not plausible. We can turn to more general variacne structures.

$$
\bf{y_{1}} = X_1\beta + Z_1b_1 + \epsilon_{1}
$$

*  ${\bf{b_1}} \sim N_3(0,\Phi)$;

* ${\bf{\epsilon_{1}}} \sim N(0,\sigma^2I_3)$.

To fit this model, we need to specify the $X_1$ and $Z_1$.

```{r}
# fixed effect X_1
Machines %>% filter(Worker == "1") %>% {model.matrix( score ~ Machine, .)}
```
```{r}
# fixed effect Z_1
Machines %>% filter(Worker == "1") %>% {model.matrix(  score ~ Machine - 1, .)}
```


```{r}
lmod.random.interact.general <- lme(score ~ Machine, Machines, random = ~ Machine -1 | Worker)
summary(lmod.random.interact.general)
```

**Model Selection**

```{r}
anova(lmod.random, lmod.random.interact, lmod.random.interact.general)
```

There's no evidence to support more general model.

# ANCOVA with random effects: `Orthodont`

```{r}
OrthFe <- Orthodont %>% filter(Sex == "Female")
knitr::kable(t(OrthFe)) %>% kable_styling("striped", "hover") %>%
  scroll_box(width = "100%", height = "100%")
```
### Modeling: Random Intercept

**Model and assumptions:** 

$$
\bf{y_{i}} = X_i\beta + Z_ib_i + \epsilon_{i}
$$

* $b_i \sim N(0,\sigma^2)$;

* $\epsilon_{i} \sim N(0,\sigma^2I)$.

For example,

```{r}
# fixed effect X_1
OrthFe  %>% filter(Subject == "F01") %>% {model.matrix( distance ~ age, .)}
```

```{r}
# random effect Z_1
OrthFe  %>% filter(Subject == "F01") %>% {model.matrix(  distance ~ 1, .)}
```

```{r}
lmod.OrthFe <- lme(distance ~ age, data=OrthFe, random = ~ 1 | Subject)
summary(lmod.OrthFe)
```

### Modeling: Random Intercept

**Model and assumptions:** 

$$
\bf{y_{i}} = X_i\beta + Z_i{\bf b_i} + \epsilon_{i}
$$

* $b_i \sim N_2(0,\Phi)$;

* $\epsilon_{i} \sim N(0,\sigma^2I)$.

**Here** $b_i=(b_{i1},b_{i2})$ and $Z_i$ is identical to $X_i$.

```{r}
lmod.OrthFe.slope <- lme(distance ~ age, data=OrthFe, random = ~ age | Subject)
summary(lmod.OrthFe.slope)
```

```{r}
anova(lmod.OrthFe, lmod.OrthFe.slope)
```

This result implies there's no explicit evidence to support the random slope model.

## Predicitons

```{r}
 knitr::kable(t(random.effects(lmod.OrthFe)))  %>% kable_styling("striped", "hover") %>%
  scroll_box(width = "100%", height = "100%")
```





# Nested Classification Factors: `Pixel`

```{r}
knitr::kable(t(Pixel)) %>% kable_styling("striped", "hover") %>%
  scroll_box(width = "100%", height = "100%")
```

```{r}
plot(Pixel)
```

## Modeling

**Model and assumptions**

$$
y_{ijk} = \beta_1 + \beta_2 d_{ik} + b_{i1} + b_{i2}d_{i,k} + b_{ij} + \epsilon_{ijk}
$$

* 

* $\epsilon_{ijk} \sim N(0,\sigma^2)$.

| parameters | explanation  |
| --- | --- |
|$\beta_1$| intercept|
|$\beta_2$| slope|
|$\sigma_b^2$|between-rail variability|
|$\epsilon_{ijk}$|noise|
|$\sigma^2$|within-rail variability|

| observation | explanation  |
| --- | --- |
| $y_{ijk}$| the k-th observation on the i-th dog's j-th side|
|$d_{ik}$|k-th obsservation on i-th dog|

* $b_{i1}$: random shift on intercept
* $b_{i2}$: random shift on slope
* $b_{ij}$: random shift on intercept for `side` within `dog` 


```{r}
lmod.nested <- lme(fixed = pixel ~ day + I(day^2), data=Pixel, random = 
                     list(Dog = pixel ~ day, Side = pixel ~ 1))
summary(lmod.nested)
```

```{r}
intervals(lmod.nested)
```


```{r}
plot(augPred(lmod.nested))
```

```{r}
lmod.nested.res1 <- lme(fixed = pixel ~ day + I(day^2), data=Pixel, random = ~ day|Dog)
summary(lmod.nested.res1)
```

```{r}
anova(lmod.nested.res1, lmod.nested)
```


```{r}
lmod.nested.res2 <- lme(fixed = pixel ~ day + I(day^2), data=Pixel, 
                        random = ~ 1|Dog/Side)
anova(lmod.nested.res2, lmod.nested)
```


# Split-plot

```{r}
plot(Oats,inner=Oats$Variety)
```

