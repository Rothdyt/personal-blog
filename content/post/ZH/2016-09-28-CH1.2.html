---
title: "无约束优化(2)"
author: Yutong Dai
date: '2016-09-28'
categories:
  - convex optimization
tags:
  - 无约束优化
slug: uncon2
output:
  blogdown::html_page:
    toc: yes
summary: 该系列posts是笔者学习Dimitri .P Bertsekas所写的 Nonlinear Programming 2ed 中文版时整理的笔记。第一章无约束优化内容涉及到最优性条件、梯度方法、牛顿法与变形以及共轭方向法。本文讨论梯度方法中下降方向和步长的选择。
---


<div id="TOC">
<ul>
<li><a>梯度方法</a><ul>
<li><a>直观分析</a></li>
<li><a>下降方向</a></li>
<li><a>步长准则</a></li>
</ul></li>
</ul>
</div>

<div class="section level2">
<h2>梯度方法</h2>
<p><span class="math display">\[outline\begin{cases}
下降方向选择\begin{cases}
            最速下降\\
            牛顿法\\
            \end{cases}\\\\
步长的选择\begin{cases}
            固定步长准则\\
            缩减步长准则\\
            最小化准则\\
            Armijo准则
            \end{cases}\\\\
收敛性分析\begin{cases}
            极限点的存在性问题\\
            迭代终止条件\\
            收敛性结论
            \end{cases}\\
收敛速率分析
\end{cases}\]</span></p>
<div class="section level3">
<h3>直观分析</h3>
</div>
<div class="section level3">
<h3>下降方向</h3>
<div class="section level4">
<h4>最速下降</h4>
</div>
<div class="section level4">
<h4>牛顿法</h4>
<p>修正的牛顿法</p>
</div>
</div>
<div class="section level3">
<h3>步长准则</h3>
<div class="section level4">
<h4>最小化准则</h4>
</div>
<div id="armijo" class="section level4">
<h4>Armijo准则</h4>
<blockquote>
<p>设<span class="math inline">\(f\)</span>是连续可微函数, <span class="math inline">\(\sigma,\beta,s\)</span>是固定的标量，其中<span class="math inline">\(0&lt;\beta&lt;1\)</span>，<span class="math inline">\(0&lt;\sigma&lt;1\)</span>,令<span class="math inline">\(\alpha_k=\beta^{m_k}s\)</span>，其中<span class="math inline">\(m_k\)</span>是满足下式的第一个非负整数m <span class="math display">\[f(x_{k+1})\leq f(x_k)+\sigma\alpha_kd_k^T\nabla f(x_k)\]</span></p>
</blockquote>
<p><strong>Remark</strong></p>
<ol style="list-style-type: decimal">
<li>准则的直观解释。考虑f(x_{k+1})在<span class="math inline">\(x_k\)</span>处的一阶Taylor 展开 <span class="math display">\[f(x_{k}+\alpha_kd_k)=f(x_k)+\alpha_kd_k^T\nabla f(x_k)+o(\alpha_k)\]</span> 由下降条件知道<span class="math inline">\(d_k^T\nabla f(x_k)&lt;0\)</span>而当<span class="math inline">\(\alpha_k\rightarrow 0\)</span>时总是存在<span class="math inline">\(\alpha_k\)</span>使得<span class="math inline">\(f(x_{k}+\alpha_kd_k)\leq f(x_k)+\alpha_kd_k^T\nabla f(x_k)\)</span>成立</li>
<li><span class="math inline">\(\sigma\)</span>保证满足准则的<span class="math inline">\(\alpha_k\)</span>可以更快的找到(<span class="math inline">\(\alpha_k\)</span>是从递减序列<span class="math inline">\(\{\beta^ms\}_{m=1}^{+\infty}\)</span>中寻找的).因此通常<span class="math inline">\(\sigma\in[10^{-5},10^{-1}]\)</span></li>
<li><span class="math inline">\(\beta\)</span>是收缩因子保证步长初始步长s可以在必要时刻减小</li>
</ol>
<p><strong>Expansion</strong></p>
<p><span class="math inline">\(F_c(x)=f(x)+cp(x)\)</span>其中<span class="math inline">\(c&gt;0\)</span>，<span class="math inline">\(f(x)\)</span>是连续可微函数，<span class="math inline">\(p(x)\)</span>是非光滑的惩罚项。我们想要极小化<span class="math inline">\(F_c(x)\)</span>，那么我们可以用梯度方法进行求解。具体来说</p>
<ol style="list-style-type: decimal">
<li>寻找下降方法<span class="math inline">\(d_k\)</span>，其中<span class="math inline">\(H_k\)</span>是<span class="math inline">\(\nabla^2 f(x_k)\)</span>的正定近似 <span class="math display">\[d_k=\mathop{argmin}_d \nabla f(x_k)^Td+\frac{1}{2}d^TH_kd+cp(x_k+d)\]</span></li>
<li>Armijo rule寻找步长<span class="math inline">\(\alpha_k\)</span></li>
</ol>
<blockquote>
<p><span class="math inline">\(0&lt;\beta&lt;1\)</span>，<span class="math inline">\(0&lt;\sigma&lt;1\)</span>,<span class="math inline">\(0\leq\gamma&lt;1\)</span>令<span class="math inline">\(\alpha_k=\beta^{m_k}s\)</span>，其中<span class="math inline">\(m_k\)</span>是满足下式的第一个非负整数m <span class="math display">\[F_c(x_k+\alpha_kd_k)\leq F_c(x_k)+\sigma\alpha_k\Delta_k\]</span> 其中<span class="math inline">\(\Delta_k=d_k^T\nabla f(x_k)+\gamma d_k^TH_kd_K+cp(x_k+d_k)-cp(x_k)\)</span></p>
</blockquote>
<p><strong>Remark</strong>: Paul Tseng et al(2007)证明了在一定条件下<span class="math inline">\(\Delta_k&lt;0\)</span></p>
</div>
<div class="section level4">
<h4>固定步长准则</h4>
</div>
<div class="section level4">
<h4>缩减步长准则</h4>
</div>
</div>
</div>
