[{"authors":null,"categories":["Statistics"],"content":" Point Anomaly Detection - Grubbs’ test Grubbs’ test1 is commonly used technique to detect an outlier in univariate problem, where normality assumption is required. It can be formualted as either one-side testing problem or two-sided testing problem.\nThe hypothesis test is defined as\n\\[H_0: \\text{There are no outlier in the data set} \\quad H_1: \\text{There is exactly one outlier in the data set}.\\] For two-sided testing, it tries to determine whether the observation with the largest absolute deviation is an outlier, where the test statistic is defined as\n\\[ G = \\frac{\\max_i |X_i - \\bar X|}{s}, \\] where the \\(\\bar X\\) is the sample mean and \\(s\\) is the sample deviation.\nLet’s look at one simulated example.\nset.seed(123) simulated_data \u0026lt;- rnorm(100, 0, 1) simulated_data_with_outliers \u0026lt;- c(simulated_data, c(3.5, -3.7)) # normality check shapiro.test(simulated_data_with_outliers) ## ## Shapiro-Wilk normality test ## ## data: simulated_data_with_outliers ## W = 0.9804, p-value = 0.1344 Shapiro-Wilk normality test impiles the data is normally distributed. Now, let’s performe the grubbs’ test.\nlibrary(outliers) grubbs.test(simulated_data_with_outliers, two.sided = TRUE) ## ## Grubbs test for one outlier ## ## data: simulated_data_with_outliers ## G = 3.65380, U = 0.86651, p-value = 0.01626 ## alternative hypothesis: lowest value -3.7 is an outlier The test result detecs the lowest value as an outlier.\nLet’s remove the -3.7 and performe the test again.\ngrubbs.test(head(simulated_data_with_outliers,101), two.sided = TRUE) ## ## Grubbs test for one outlier ## ## data: head(simulated_data_with_outliers, 101) ## G = 3.48190, U = 0.87755, p-value = 0.03378 ## alternative hypothesis: highest value 3.5 is an outlier Finally, let’s remove two outliers altogether.\ngrubbs.test(head(simulated_data_with_outliers,100), two.sided = TRUE) ## ## Grubbs test for one outlier ## ## data: head(simulated_data_with_outliers, 100) ## G = 2.62880, U = 0.92949, p-value = 0.7584 ## alternative hypothesis: lowest value -2.30916887564081 is an outlier This impiles there are no outliers.\nGrubbs’ test is useful for identify the outliers of a small amount one at a time, but not suitable to detect a group of outliers.\n Collective Anomaly Detection Anomaly in timeseries - Seasonal Hybrid ESD algorithm # devtools::install_github(\u0026quot;twitter/AnomalyDetection\u0026quot;) library(AnomalyDetection) river \u0026lt;- read.csv(\u0026quot;https://raw.githubusercontent.com/Rothdyt/personal-blog/master/static/files/post_data/river.csv\u0026quot;) results \u0026lt;- AnomalyDetectionVec(river$nitrate, period=12, direction = \u0026#39;both\u0026#39;, plot = T) results$plot  Distance-based Anomaly Detection Global Anomaly - Largest Distance Intuitively, the larger distance the more likely the point would be an outlier.\nlibrary(FNN) ## Warning: package \u0026#39;FNN\u0026#39; was built under R version 3.4.4 furniture \u0026lt;- read.csv(\u0026quot;https://raw.githubusercontent.com/Rothdyt/personal-blog/master/static/files/post_data/furniture.csv\u0026quot;) furniture_scaled \u0026lt;- data.frame(Height = scale(furniture$Height), Width = scale(furniture$Width)) furniture_knn \u0026lt;- get.knn(furniture_scaled, k = 5) furniture_scaled$score_knn \u0026lt;- rowMeans(furniture_knn$nn.dist) largest_idx \u0026lt;- which.max(furniture_scaled$score_knn) plot(furniture_scaled$Height, furniture_scaled$Width, cex=sqrt(furniture_scaled$score_knn), pch=20) points(furniture_scaled$Height[largest_idx], furniture_scaled$Width[largest_idx], col=\u0026quot;red\u0026quot;, pch=20)  Local Anomaly - LOF kNN is useful for finding global anomalies, but is less able to surface local outliers.\nLOF is a ratio of densities:\n LOF \u0026gt; 1 more likely to be anomalous LOF ≤ 1 less likely to be anomalous Large LOF values indicate more isolated points  library(dbscan) furniture_lof \u0026lt;- furniture[,2:3] furniture_lof$score_lof \u0026lt;- lof(scale(furniture_lof), k=5) largest_idx \u0026lt;- which.max(furniture_lof$score_lof) plot(furniture_lof$Height, furniture_lof$Width, cex=sqrt(furniture_lof$score_lof), pch=20) points(furniture_lof$Height[largest_idx], furniture_lof$Width[largest_idx], col=\u0026quot;red\u0026quot;, pch=20) It’s clear that, lof successfuly detects the local outlier.\n   Isolation Forest  Isolation Forest is built on the basis of decision trees; To grow a decision tree, at each node, a feature and a corresponding cutoff value are randomly selected; Intuitively, outliers are less frequent than regular observations and are different from them in terms of values, so outliers should be identified closer to the root of the tree with fewer splits. We use isolation score to characterize this.  Isolation Score We need some quatity to define the isolation score2\n Path Length: \\(h(x)\\) of a point \\(x\\) is measured by the number of edges \\(x\\) traverses an iTree from the root node until the traversal is terminated at an external node. Normalizing constant \\[c(n) = 2H(n − 1) − (2(n − 1)/n)\\], where \\(n\\) is the number of samples to grow a tree and \\(H(i)\\) is the harmonic number and it can be estimated by \\(ln(i) + 0.5772156649\\) (Euler’s constant).  The isolation score \\(s\\) of an sample \\(x\\) is defined as \\[s(x,n)= 2^{-\\frac{E(h(x))}{c(n)}},\\] where the \\(E()\\) is the expectation of \\(h(x)\\).\n Interpreting the isolation score:\n Scores between 0 and 1 Scores near 1 indicate anomalies (small path length)\n  # devtools::install_github(\u0026quot;Zelazny7/isofor\u0026quot;) library(isofor) furniture \u0026lt;- read.csv(\u0026quot;https://raw.githubusercontent.com/Rothdyt/personal-blog/master/static/files/post_data/furniture.csv\u0026quot;) furniture \u0026lt;- data.frame(Height = furniture$Height, Width = furniture$Width) scores \u0026lt;- matrix(nrow=dim(furniture)[1]) for (ntree in c(100, 200, 500)){ furniture_tree \u0026lt;- iForest(furniture, nt = ntree, phi=50) scores \u0026lt;- cbind(scores, predict(furniture_tree, furniture)) } plot(scores[,3], scores[,4], xlab = \u0026quot;200 tress\u0026quot;, ylab=\u0026quot;500 tress\u0026quot;) abline(a=0,b=1) This graph is used to assess wheter the number of trees is enough for the isolation score to converge. From the graph above, we know that 200 tress are enough for us to identify the anomalies.\nlibrary(lattice) furniture_forest \u0026lt;- iForest(furniture, nt = 200, phi=50) h_seq \u0026lt;- seq(min(furniture$Height), max(furniture$Height), length.out = 20) w_seq \u0026lt;- seq(min(furniture$Width), max(furniture$Width), length.out = 20) furniture_grid \u0026lt;- expand.grid(Width = w_seq, Height = h_seq) furniture_grid$score \u0026lt;- predict(furniture_forest, furniture_grid) contourplot(score ~ Height + Width, data = furniture_grid,region = TRUE) This contour graph used to identify the anomaly regions.\n   https://en.wikipedia.org/wiki/Grubbs%27_test_for_outliers↩\n Zhihua Zhou et al. https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf↩\n   ","date":1546560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546560000,"objectID":"15a31f7a48224df1923c4da5f192ad5d","permalink":"/post/anomaly-detection/","publishdate":"2019-01-04T00:00:00Z","relpermalink":"/post/anomaly-detection/","section":"post","summary":"Point Anomaly Detection - Grubbs’ test Grubbs’ test1 is commonly used technique to detect an outlier in univariate problem, where normality assumption is required. It can be formualted as either one-side testing problem or two-sided testing problem.\nThe hypothesis test is defined as\n\\[H_0: \\text{There are no outlier in the data set} \\quad H_1: \\text{There is exactly one outlier in the data set}.\\] For two-sided testing, it tries to determine whether the observation with the largest absolute deviation is an outlier, where the test statistic is defined as","tags":["data-analysis"],"title":"Anomaly Detection","type":"post"},{"authors":null,"categories":null,"content":" Manhattan 2018.12 I went to the New York for vacation. And this was the time I learned how to edit the video clips.\n ","date":1545541200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545541200,"objectID":"cf041cf37e12b52e354fe54b5dc9bda3","permalink":"/gallery/movies/","publishdate":"2018-12-23T00:00:00-05:00","relpermalink":"/gallery/movies/","section":"gallery","summary":"Record the momements.","tags":["Videos","NYC"],"title":"Trip to New York","type":"gallery"},{"authors":null,"categories":["Python-Programming"],"content":"  Prepare a fitted random forest Find the path to desired terminal node Collect Paths in the random forest Summarize the decison region   Prepare a fitted random forest import random import pandas as pd from sklearn.ensemble.forest import RandomForestRegressor from sklearn import tree data = pd.DataFrame({\u0026quot;Y\u0026quot;:[1,5,3,4,3,4,2], \u0026quot;X_1\u0026quot;:[\u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;, \u0026quot;blue\u0026quot;, \u0026quot;red\u0026quot;,\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;, \u0026quot;red\u0026quot;], \u0026quot;X_2\u0026quot;:[18.4, 7.5, 9.3, 3.7, 5.2, 3.2, 5.2]}) data = pd.get_dummies(data) X = data.drop([\u0026quot;Y\u0026quot;], axis=1) y = data[\u0026quot;Y\u0026quot;] rf = RandomForestRegressor(n_estimators = 10, random_state = 1234) rf.fit(X, y) output:\nRandomForestRegressor(bootstrap=True, criterion=\u0026#39;mse\u0026#39;, max_depth=None, max_features=\u0026#39;auto\u0026#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1, oob_score=False, random_state=1234, verbose=0, warm_start=False)  Find the path to desired terminal node import pydotplus import re def return_node_path_to_max_prediction(onetree, verbose=True): \u0026quot;\u0026quot;\u0026quot; @input: a tree from the sklearn randomforest @output: the node path to maxmium terminal node [[split_node_1], [split_node_2], ...] [splite_node_1] = [var_index, cutoff, direction] \u0026quot;\u0026quot;\u0026quot; if verbose: print(\u0026quot;Generating Tree Graph, it may take a while...\u0026quot;) dot_data = tree.export_graphviz(onetree, out_file = None, filled = True, rounded = True, special_characters = True) graph = pydotplus.graph_from_dot_data(dot_data) graph_ = {} for edge in graph.get_edge_list(): graph_[edge.get_source()] = edge.get_destination() # find all terminal node terminal_node = {} non_decimal = re.compile(r\u0026#39;[^\\d.]+\u0026#39;) for node in graph.get_node_list(): if node.get_name() not in graph_: if node.get_name() not in [\u0026quot;node\u0026quot;, \u0026quot;edge\u0026quot;]: value = node.get_label() value = re.sub(r\u0026#39;.*v\u0026#39;, \u0026#39;v\u0026#39;, value) terminal_node[node.get_name()] = float(non_decimal.sub(\u0026#39;\u0026#39;, value)) # find the path down to the terminal with maximum predition value flag = True destination = max(terminal_node, key=terminal_node.get) edge_list = graph.get_edge_list() node_list = graph.get_node_list() split_node = [] while flag: myedge = [edge for edge in edge_list if edge.get_destination() == destination][0] if int(myedge.get_destination()) - int(myedge.get_source()) \u0026gt; 1: direction = \u0026quot;Right\u0026quot; else: direction = \u0026quot;Left\u0026quot; mynode = [node for node in node_list if node.get_name() == myedge.get_source()][0] var_val = re.findall(r\u0026quot;[-+]?\\d*\\.\\d+|\\d+\u0026quot;, mynode.get_label())[:2] # record the growing path: # var_val[0]: Index of variable participating in splitting # var_val[1]: cutoff point of the splitting # direction: If Right, means greater than var_val[1]; # If Left, means no greater than var_val[1] split_node.append([int(var_val[0]),float(var_val[1]),direction]) if verbose: print(myedge.get_destination() + \u0026quot;\u0026lt;-\u0026quot; + myedge.get_source() + \u0026quot;: Split at Variable X\u0026quot; + var_val[0] + \u0026quot;; The cutoff is \u0026quot; + var_val[1] + \u0026quot;; Turn \u0026quot; + direction) destination = myedge.get_source() if destination == \u0026quot;0\u0026quot;: flag = False return [*reversed(split_node)] Test:\nreturn_node_path_to_max_prediction(rf[1], verbose=True) Outputs:\nGenerating Tree Graph, it may take a while... 3\u0026lt;-1: Split at Variable X0; The cutoff is 5.6; Turn Right 1\u0026lt;-0: Split at Variable X0; The cutoff is 12.95; Turn Left From the output above, we know the path from the root to the desired terminal node is :\nRoot[X0(\u0026lt;= 12.95)] -\u0026gt; X0 (\u0026gt;=5.6) -\u0026gt; Terminal Node\n Collect Paths in the random forest def collect_path(rf, verbose=True): n_tree = len(rf) result = [] for i in range(n_tree): if verbose: print(\u0026quot;Construct the %s tree graph out of %s trees\u0026quot; %(i+1, n_tree)) result.append(return_node_path_to_max_prediction(rf.estimators_[i], verbose=False)) return result Test:\nresult = collect_path(rf) print(result) Outputs:\nConstruct the 1 tree graph out of 10 trees Construct the 2 tree graph out of 10 trees Construct the 3 tree graph out of 10 trees Construct the 4 tree graph out of 10 trees Construct the 5 tree graph out of 10 trees Construct the 6 tree graph out of 10 trees Construct the 7 tree graph out of 10 trees Construct the 8 tree graph out of 10 trees Construct the 9 tree graph out of 10 trees Construct the 10 tree graph out of 10 trees [[[0, 4.2, \u0026#39;Left\u0026#39;]], [[0, 12.95, \u0026#39;Left\u0026#39;], [0, 5.6, \u0026#39;Right\u0026#39;]], [[1, 0.5, \u0026#39;Right\u0026#39;], [0, 8.4, \u0026#39;Left\u0026#39;]], [[0, 13.85, \u0026#39;Left\u0026#39;], [0, 8.4, \u0026#39;Left\u0026#39;], [1, 0.5, \u0026#39;Right\u0026#39;]], [[0, 8.4, \u0026#39;Left\u0026#39;], [0, 6.35, \u0026#39;Right\u0026#39;]], [[0, 12.95, \u0026#39;Left\u0026#39;], [0, 5.6, \u0026#39;Right\u0026#39;]], [[2, 0.5, \u0026#39;Left\u0026#39;], [0, 5.35, \u0026#39;Right\u0026#39;]], [[1, 0.5, \u0026#39;Right\u0026#39;], [0, 5.35, \u0026#39;Right\u0026#39;]], [[0, 13.85, \u0026#39;Left\u0026#39;], [1, 0.5, \u0026#39;Right\u0026#39;], [0, 8.4, \u0026#39;Left\u0026#39;], [0, 5.35, \u0026#39;Right\u0026#39;]], [[0, 13.85, \u0026#39;Left\u0026#39;], [0, 6.35, \u0026#39;Right\u0026#39;], [0, 8.4, \u0026#39;Left\u0026#39;]]]  Summarize the decison region def summarize_region(result, features): decision_region = {k: [[] for _ in range(2)] for k in features} for i in range(len(result)): for j in range(len(result[i])): if result[i][j][2] == \u0026quot;Left\u0026quot;: decision_region[features[result[i][j][0]]][0].append(result[i][j][1]) else: decision_region[features[result[i][j][0]]][1].append(result[i][j][1]) decision_region_ = {} for k in features: try: upper_bound = min(decision_region[k][0]) except ValueError: upper_bound = \u0026quot;Unknown\u0026quot; try: lower_bound = max(decision_region[k][1]) except ValueError: lower_bound = \u0026quot;Unknown\u0026quot; decision_region_[k] = [lower_bound, upper_bound] value_to_remove = [\u0026#39;Unknown\u0026#39;, \u0026#39;Unknown\u0026#39;] decision_region_ = {key: value for key, value in decision_region_.items() if value != value_to_remove} value_to_remove = [0.5, 0.5] decision_region_ = {key: value for key, value in decision_region_.items() if value != value_to_remove} return (decision_region_) Test:\nfeatures = X.columns summarize_region(result, features) Outputs:\n{\u0026#39;X_1_blue\u0026#39;: [0.5, \u0026#39;Unknown\u0026#39;], \u0026#39;X_1_red\u0026#39;: [\u0026#39;Unknown\u0026#39;, 0.5], \u0026#39;X_2\u0026#39;: [6.35, 4.2]} From the output above, we know that the decision region:\n{blue} * [6.35, 4.2] But it seems that the region [6.35, 4.2] is not reasonable due to the poorly generated data. But it may happens in some situations, which may require us to come up with new ways to ensemble these terminal nodes.\n ","date":1530921600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530921600,"objectID":"d2ed08f4b3ca1950ddad9267c40712e1","permalink":"/post/decision-tree-how-to-find-the-path-from-the-root-to-the-desired-terminal-node/","publishdate":"2018-07-07T00:00:00Z","relpermalink":"/post/decision-tree-how-to-find-the-path-from-the-root-to-the-desired-terminal-node/","section":"post","summary":"Find terminal nodes in each tree of the built random forest that give the largest prediction. Then find paths from root to these selected terminal nodes and ensemble them to derive a decision region.","tags":["RandomForest"],"title":"Decision Tree: How to find the path from the root to the desired terminal node","type":"post"},{"authors":null,"categories":["R-Programming"],"content":"  Motivation Solution Slicing Basic linear algebra operations    Motivation Recently, I have to deal with matrices with sparsity structures. If I coerce them using the as.matrix function, they consume my RAM wildly. So I turn to the package SparseM for possible solutions.\nWhat I want to do are,\nPerforme basic linear algebra operations on sparse matrices, mainly including multiplication, transpose, and etc. Slice rows or columns by their indices.  The package SparseM can perfectly fulfill my needs.\n Solution set.seed(1234) A \u0026lt;- matrix(rnorm(16), ncol=4, nrow=4) A[A \u0026lt; 0] \u0026lt;- 0 A ## [,1] [,2] [,3] [,4] ## [1,] 0.0000000 0.4291247 0 0.00000000 ## [2,] 0.2774292 0.5060559 0 0.06445882 ## [3,] 1.0844412 0.0000000 0 0.95949406 ## [4,] 0.0000000 0.0000000 0 0.00000000 We reformat A in csr form:\nsuppressMessages(library(SparseM)) A.csr \u0026lt;- as.matrix.csr(A) A.csr ## An object of class \u0026quot;matrix.csr\u0026quot; ## Slot \u0026quot;ra\u0026quot;: ## [1] 0.42912469 0.27742924 0.50605589 0.06445882 1.08444118 0.95949406 ## ## Slot \u0026quot;ja\u0026quot;: ## [1] 2 1 2 4 1 4 ## ## Slot \u0026quot;ia\u0026quot;: ## [1] 1 2 5 7 7 ## ## Slot \u0026quot;dimension\u0026quot;: ## [1] 4 4  ra: a real array of non-zero elements containing the non-zero elements of A, stored in row order. Thus, if i \u0026lt; j, all elements of row i precede elements from row j. The order of elements within the rows is immaterial.\n  ja: an integer array of non-zero elements containing the column indices of the elements stored in ra.\n  ia: an integer array of n+1 elements containing pointers to the beginning of each row in the arrays ra and ja. Thus ia[i] indicates the position in the arrays ra and ja where the ith row begins. The last (n + 1)st element of ia indicates where the n + 1 row would start, if it existed.\n Slicing A.csr[,1:2] ## An object of class \u0026quot;matrix.csr\u0026quot; ## Slot \u0026quot;ra\u0026quot;: ## [1] 0.4291247 0.2774292 0.5060559 1.0844412 ## ## Slot \u0026quot;ja\u0026quot;: ## [1] 2 1 2 1 ## ## Slot \u0026quot;ia\u0026quot;: ## [1] 1 2 4 5 5 ## ## Slot \u0026quot;dimension\u0026quot;: ## [1] 4 2 as.matrix(A.csr[,1:2]) ## [,1] [,2] ## [1,] 0.0000000 0.4291247 ## [2,] 0.2774292 0.5060559 ## [3,] 1.0844412 0.0000000 ## [4,] 0.0000000 0.0000000 A.csr[, -1] ## An object of class \u0026quot;matrix.csr\u0026quot; ## Slot \u0026quot;ra\u0026quot;: ## [1] 0.42912469 0.50605589 0.06445882 0.95949406 ## ## Slot \u0026quot;ja\u0026quot;: ## [1] 1 1 3 3 ## ## Slot \u0026quot;ia\u0026quot;: ## [1] 1 2 4 5 5 ## ## Slot \u0026quot;dimension\u0026quot;: ## [1] 4 3 as.matrix(A.csr[, -1]) ## [,1] [,2] [,3] ## [1,] 0.4291247 0 0.00000000 ## [2,] 0.5060559 0 0.06445882 ## [3,] 0.0000000 0 0.95949406 ## [4,] 0.0000000 0 0.00000000  Basic linear algebra operations t(A.csr) ## An object of class \u0026quot;matrix.csr\u0026quot; ## Slot \u0026quot;ra\u0026quot;: ## [1] 0.27742924 1.08444118 0.42912469 0.50605589 0.06445882 0.95949406 ## ## Slot \u0026quot;ja\u0026quot;: ## [1] 2 3 1 2 2 3 ## ## Slot \u0026quot;ia\u0026quot;: ## [1] 1 3 5 5 7 ## ## Slot \u0026quot;dimension\u0026quot;: ## [1] 4 4 as.matrix(A.csr %*% c(1,2,3,4)) ## [,1] ## [1,] 0.8582494 ## [2,] 1.5473763 ## [3,] 4.9224174 ## [4,] 0.0000000 A %*% c(1,2,3,4) ## [,1] ## [1,] 0.8582494 ## [2,] 1.5473763 ## [3,] 4.9224174 ## [4,] 0.0000000 We can even visualize its sparse structure:\nset.seed(1234) A \u0026lt;- matrix(rnorm(2000), ncol=50, nrow=40) A[abs(A) \u0026lt; 0.7] \u0026lt;- 0 image(as.matrix.csr(A))   ","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"a4e2f1fd9569e516df0e5101ba100c15","permalink":"/post/play-with-sparse-matrix/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/post/play-with-sparse-matrix/","section":"post","summary":"Play with sparse matirx using `SparseM`.","tags":["R-tricks"],"title":"Play with Sparse Matrix","type":"post"},{"authors":null,"categories":["R-Programming"],"content":"  Motivation Solutions Download Uncompress data files Load libsvm format data Complete Code    Motivation I’m using the datasets libsvm to do some numerical experiments. The sizes of datasets, like epsilon1, are of severalGBs. So it is hard to play with the data on my desktop. I am lucky to run the experiments on a server. So my workflow is:\nDownload datasets on the server. Preprocess datastes. Run experiments.  The problems I encounted are\nHow to use R functions to download data? Since dataset files are compressed, how to uncompresse them? How to read dataset are of the libsvm format?   Solutions If you only care about the way to download datasets from R, then you can stop once finish reading the following subsection.\nDownload Use built-in function download.file, one can finish the task.\ndownload.file(target_path, path_filename_extension) For example,\ndownload.file(https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/epsilon_normalized.bz2, \u0026quot;./data/epsilon_normalized.bz2\u0026quot;)  Uncompress data files Changce are that you datasets are compressed in .zip, .bz2 and etc. formats, so you cannot directly load your data into the memory. You can use the function bunzip2 from the R.utils packages.\nbunzip2(path_filet_to_uncompress, path_file_to_save, remove = FALSE, skip = TRUE)  remove: If TRUE, the compressed file will be removed once uncompression finished. skip: If TRUE and the output file already exists, the output file is returned as is.  For example,\ndownload.file(\u0026quot;./data/epsilon_normalized.bz2\u0026quot;, \u0026quot;./data/epsilon_normalized\u0026quot;) Here, the original dataset file epsilon_normalized has no extension.\n Load libsvm format data The libsvm format data is of form\n\u0026lt;label\u0026gt; \u0026lt;index 1\u0026gt;:\u0026lt;value 1\u0026gt; \u0026lt;index 2\u0026gt;:\u0026lt;value 2\u0026gt; ... \u0026lt;index n\u0026gt;:\u0026lt;value n\u0026gt; For example,\n1 1:12 2:34 3:56 2 1:98 2:76 3:65 For more information, please refer to this post.\nWe can use the read.matrix.csr function from the e1071 package.\ndata \u0026lt;- read.matrix.csr(file, fac=TRUE) For example,\ndata \u0026lt;- read.matrix.csr(\u0026quot;./data/epsilon_normalized\u0026quot;, fac=TRUE) Note that data here is a list and matrix are stored in sparse matrix format. We can use data$x and data$y to extract the feature matrix and the label vector respectively.\n Complete Code The complete code to download and uncompress data files is\nget_data \u0026lt;- function(target_path, save_path, save_name, extension, unzip=TRUE){ path_file \u0026lt;- paste(save_path,save_name,sep=\u0026quot;/\u0026quot;) path_file_extension \u0026lt;- paste(path_file, extension, sep=\u0026quot;.\u0026quot;) download.file(target_path, path_file_extension) if (unzip){ require(R.utils) bunzip2(path_file_extension, path_file, remove = FALSE, skip = TRUE) print(\u0026quot;unzip finished!\u0026quot;) } print(paste(\u0026quot;data is strored at:\u0026quot;, save_path)) } The complete code to load libsvm format data is\nprocess_data \u0026lt;- function(file, convert_to_factor=FALSE){ require(e1071) require(SparseM) print(\u0026quot;loading data\u0026quot;) data \u0026lt;- read.matrix.csr(file, fac=convert_to_factor) print(\u0026quot;loading finished\u0026quot;) x \u0026lt;- as.matrix(data$x) print(\u0026quot;x was conveted to dense matrix form\u0026quot;) y \u0026lt;- as.matrix(data$y) results \u0026lt;- list(x=x,y=y) return(results) }    https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/epsilon_normalized.bz2↩\n   ","date":1527724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527724800,"objectID":"ff1b1512ead122ed428bde8fe4e3a979","permalink":"/post/get-data-from-website/","publishdate":"2018-05-31T00:00:00Z","relpermalink":"/post/get-data-from-website/","section":"post","summary":"Use `R` funuctions to download datasets from websites and read `libsvm` formatted data.","tags":["R-tricks"],"title":"Get Data from the Website","type":"post"},{"authors":null,"categories":["R-writing"],"content":"  Math Equations with labels    Following contents are adapeted from Yihui Xie’s bookdown\n Math Equations with labels My personal preference is to use algin and equation for long or complicated and simple expressions respectively. The slight differeces, compared with the \\(\\bf \\LaTeX\\) syntax, are the way we assign labels to expressions. Instead of using \\label{foo}, in blogdown (bookdown), it is recommend to use the syntax (\\#eq:foo).\nFollowing are two simple examples. Use\n\\begin{equation} f(k) = \\binom{n}{k} p^k(1-p)^{n-k} (\\#eq:binom) \\end{equation} to produce\n\\[\\begin{equation} f(k) = \\binom{n}{k} p^k(1-p)^{n-k} \\tag{1} \\end{equation}\\] Similarly, we can use \u0026amp; to algin equations.\n\\begin{align} \u0026amp; f(x;\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}}\\exp(\\frac{1}{2\\sigma^2}(x-\\mu)^2) (\\#eq:gaussian)\\\\ \u0026amp; f(x;\\lambda) = \\lambda e^{-\\lambda}I(x\u0026gt;0) (\\#eq:exp) \\end{align} \\[\\begin{align} \u0026amp; f(x;\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}}\\exp(\\frac{1}{2\\sigma^2}(x-\\mu)^2) \\tag{2}\\\\ \u0026amp; f(x;\\lambda) = \\lambda e^{-\\lambda}I(x\u0026gt;0) \\tag{3} \\end{align}\\] For HTML output, bookdown can only number the equations with labels. Please make sure equations without labels are not numbered by either using the equation* environment or adding \\nonumber or \\notag to your equations. The same rules apply to other math environments, such as eqnarray, gather, align, and so on (e.g., you can use the align* environment).\nTo refer to a equation, please use \\@ref(eq:label). For example,\n\\@ref(eq:binom) can produce its index (1).\n ","date":1527465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527465600,"objectID":"45f71813b44d9d7c4336be5a73b02045","permalink":"/post/bookdown-syntax/","publishdate":"2018-05-28T00:00:00Z","relpermalink":"/post/bookdown-syntax/","section":"post","summary":"Introduce basic `bookdown` syntax for scientific articles.","tags":["bookdown"],"title":"Bookdown Syntax","type":"post"},{"authors":null,"categories":null,"content":"","date":1524801600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524801600,"objectID":"057b058272a2ed8160dd4f6325ace151","permalink":"/gallery/sketches/","publishdate":"2018-04-27T00:00:00-04:00","relpermalink":"/gallery/sketches/","section":"gallery","summary":"Some interesing sketches for leisure.","tags":["Sketches"],"title":"Sketches","type":"gallery"},{"authors":null,"categories":["Statistics"],"content":"   Why Ramdom Effect Models One-way classification with Rail example Rail Data Modeling Fitting the model with the package nlme Diagnostic  Two-way classification: A Randomized Block Design egroStool: No Replications Modeling Diagnostic  Machines: Replications Modeling Model Selection  Extensions for interaction terms  ANCOVA with random effects: Orthodont Modeling: Random Intercept Modeling: Random Intercept Predicitons  Nested Classification Factors: Pixel Modeling  Split-plot    References: Pinheiro, J. C., \u0026amp; Bates, D. (2009). Mixed-Effects Models in S and S-PLUS. Statistics and Computing, Springer.\n Why Ramdom Effect Models Compared with the linear fixed effect models, the advantages of modeling data with linear mixed-effect models (MLE) are:\nRather than models the specific type of sample of levels from the factor of interest, e.g Rail tyep I and Rail type II sampled from all possible types of the Rail, LME models the factor from the population perspective.\n Provide an estimate of the between-level variability.\n Prevent the model complexity (# of parameters) increases linearly with the increase with # of levels.\n   One-way classification with Rail example Rail Data suppressMessages(library(nlme)) suppressMessages(library(kableExtra)) knitr::kable(t(Rail)) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;) %\u0026gt;% scroll_box(width = \u0026quot;100%\u0026quot;, height = \u0026quot;100%\u0026quot;)    1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18      Rail  1  1  1  2  2  2  3  3  3  4  4  4  5  5  5  6  6  6    travel  55  53  54  26  37  32  78  91  85  92  100  96  49  51  50  80  85  83      plot(Rail)  Modeling Model and assumptions \\[ y_{ij} = \\mu + b_i + \\epsilon_{ij} \\]\n \\(b_i \\sim N(0,\\sigma_b^2)\\);\n \\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\).\n    parameters explanation    \\(\\mu\\) population mean  \\(\\sigma_b^2\\) between-rail variability  \\(\\epsilon_{ij}\\) noise  \\(\\sigma^2\\) within-rail variability    \\(b_i\\) represents the deviation of the i-th type of rail from the population, it is not a parameter. Note this is a balcance design.   Fitting the model with the package nlme rail_data \u0026lt;- Rail lm.random \u0026lt;- lme(fixed = travel~1, data=rail_data, random = ~1 | Rail) # fixed = travel~1: only the intercept is the fixed effect # random = ~1 | Rail: there is a single random effect for each group and the grouping is given by the Rail summary(lm.random) ## Linear mixed-effects model fit by REML ## Data: rail_data ## AIC BIC logLik ## 128.177 130.6766 -61.0885 ## ## Random effects: ## Formula: ~1 | Rail ## (Intercept) Residual ## StdDev: 24.80547 4.020779 ## ## Fixed effects: travel ~ 1 ## Value Std.Error DF t-value p-value ## (Intercept) 66.5 10.17104 12 6.538173 0 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -1.61882658 -0.28217671 0.03569328 0.21955784 1.61437744 ## ## Number of Observations: 18 ## Number of Groups: 6  \\(\\hat\\mu =\\) 66.5 \\(\\hat\\sigma_b^2=24.80547\\) \\(\\hat\\sigma^2=\\) 4.0207794   Diagnostic Confidence Intervals on parameters:\nintervals(lm.random) ## Approximate 95% confidence intervals ## ## Fixed effects: ## lower est. upper ## (Intercept) 44.33921 66.5 88.66079 ## attr(,\u0026quot;label\u0026quot;) ## [1] \u0026quot;Fixed effects:\u0026quot; ## ## Random Effects: ## Level: Rail ## lower est. upper ## sd((Intercept)) 13.27434 24.80547 46.35341 ## ## Within-group standard error: ## lower est. upper ## 2.695007 4.020779 5.998747 The estimation of \\(\\mu,\\sigma_b,\\sigma\\) are really raw and imprecise.\nANOVA of the fixed effect:\nanova(lm.random) ## numDF denDF F-value p-value ## (Intercept) 1 12 42.7477 \u0026lt;.0001   Two-way classification: A Randomized Block Design For block design, we have two types of factors, one is called experimental factor for fixed effects while the other is called blocking factor for the random effects.\negroStool: No Replications We want to make inferences subjects and types of stools are blocking factors.\nknitr::kable(t(ergoStool)) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;) %\u0026gt;% scroll_box(width = \u0026quot;100%\u0026quot;, height = \u0026quot;100%\u0026quot;)    1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36      effort  12  15  12  10  10  14  13  12  7  14  13  9  7  11  10  9  8  11  8  7  9  11  11  10  8  12  12  11  7  11  8  7  9  13  10  8    Type  T1  T2  T3  T4  T1  T2  T3  T4  T1  T2  T3  T4  T1  T2  T3  T4  T1  T2  T3  T4  T1  T2  T3  T4  T1  T2  T3  T4  T1  T2  T3  T4  T1  T2  T3  T4    Subject  1  1  1  1  2  2  2  2  3  3  3  3  4  4  4  4  5  5  5  5  6  6  6  6  7  7  7  7  8  8  8  8  9  9  9  9      plot(ergoStool) Modeling Model and assumptions: \\[ y_{ij} = \\beta_j + b_i + \\epsilon_{ij} \\]\n \\(b_i \\sim N(0,\\sigma_b^2)\\);\n \\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\).\n    parameters explanation    \\(\\beta_j\\) experimental factor (researchers are interested in)  \\(\\sigma_b^2\\) between-rail variability  \\(\\epsilon_{ij}\\) noise  \\(\\sigma^2\\) within-rail variability     Note this is a balcance design  Choose the contrasts ergoStool %\u0026gt;% filter(Subject == \u0026quot;1\u0026quot;) %\u0026gt;% model.matrix(effort ~ Type, .) ## Warning: package \u0026#39;bindrcpp\u0026#39; was built under R version 3.4.4 ## (Intercept) TypeT2 TypeT3 TypeT4 ## 1 1 0 0 0 ## 2 1 1 0 0 ## 3 1 0 1 0 ## 4 1 0 0 1 ## attr(,\u0026quot;assign\u0026quot;) ## [1] 0 1 1 1 ## attr(,\u0026quot;contrasts\u0026quot;) ## attr(,\u0026quot;contrasts\u0026quot;)$Type ## [1] \u0026quot;contr.treatment\u0026quot; lmod.random \u0026lt;- lme(effort ~ Type, data = ergoStool, random = ~1 | Subject) summary(lmod.random) ## Linear mixed-effects model fit by REML ## Data: ergoStool ## AIC BIC logLik ## 133.1308 141.9252 -60.56539 ## ## Random effects: ## Formula: ~1 | Subject ## (Intercept) Residual ## StdDev: 1.332465 1.100295 ## ## Fixed effects: effort ~ Type ## Value Std.Error DF t-value p-value ## (Intercept) 8.555556 0.5760123 24 14.853079 0.0000 ## TypeT2 3.888889 0.5186838 24 7.497610 0.0000 ## TypeT3 2.222222 0.5186838 24 4.284348 0.0003 ## TypeT4 0.666667 0.5186838 24 1.285304 0.2110 ## Correlation: ## (Intr) TypeT2 TypeT3 ## TypeT2 -0.45 ## TypeT3 -0.45 0.50 ## TypeT4 -0.45 0.50 0.50 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -1.80200345 -0.64316591 0.05783115 0.70099706 1.63142054 ## ## Number of Observations: 36 ## Number of Groups: 9  \\(\\hat\\beta_1=\\)intercept: reference level(TypeT1); \\(\\hat\\beta_i=\\) (TypeTi) + intercept i=2,3,4  Significance Tests for fixed effects \\(\\beta_2=\\beta_3=\\beta_4=0\\)\nanova(lmod.random) ## numDF denDF F-value p-value ## (Intercept) 1 24 455.0075 \u0026lt;.0001 ## Type 3 24 22.3556 \u0026lt;.0001 \\(P(F_{3,24} \u0026gt; 22.3556)\u0026lt;.0001\\), there are significance variation within the each type of stools.\n  Diagnostic intervals(lmod.random) ## Approximate 95% confidence intervals ## ## Fixed effects: ## lower est. upper ## (Intercept) 7.3667247 8.5555556 9.744386 ## TypeT2 2.8183781 3.8888889 4.959400 ## TypeT3 1.1517114 2.2222222 3.292733 ## TypeT4 -0.4038442 0.6666667 1.737177 ## attr(,\u0026quot;label\u0026quot;) ## [1] \u0026quot;Fixed effects:\u0026quot; ## ## Random Effects: ## Level: Subject ## lower est. upper ## sd((Intercept)) 0.749509 1.332465 2.368835 ## ## Within-group standard error: ## lower est. upper ## 0.8292494 1.1002946 1.4599324 plot(lmod.random, form = resid(., type=\u0026quot;p\u0026quot;) ~ fitted(.) | Subject, abline = 0)   Machines: Replications With replications, we are able to access the interactions between the experimental and blocking effects!!\nknitr::kable(t(Machines)) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;) %\u0026gt;% scroll_box(width = \u0026quot;100%\u0026quot;, height = \u0026quot;100%\u0026quot;)    1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54      Worker  1  1  1  2  2  2  3  3  3  4  4  4  5  5  5  6  6  6  1  1  1  2  2  2  3  3  3  4  4  4  5  5  5  6  6  6  1  1  1  2  2  2  3  3  3  4  4  4  5  5  5  6  6  6    Machine  A  A  A  A  A  A  A  A  A  A  A  A  A  A  A  A  A  A  B  B  B  B  B  B  B  B  B  B  B  B  B  B  B  B  B  B  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C    score  52.0  52.8  53.1  51.8  52.8  53.1  60.0  60.2  58.4  51.1  52.3  50.3  50.9  51.8  51.4  46.4  44.8  49.2  62.1  62.6  64.0  59.7  60.0  59.0  68.6  65.8  69.7  63.2  62.8  62.2  64.8  65.0  65.4  43.7  44.2  43.0  67.5  67.2  66.9  61.5  61.7  62.3  70.8  70.6  71.0  64.1  66.2  64.0  72.1  72.0  71.1  62.0  61.4  60.5      Machines %\u0026gt;% {plot.experiment(.$score,.$Machine,.$Worker, lab.x = \u0026quot;Score\u0026quot;, lab.y = \u0026quot;Worker\u0026quot;, lab.experimental = \u0026quot;Machine\u0026quot;)} Modeling Model and assumptions - without interactions:\n\\[ y_{ijk} = \\beta_j + b_i + \\epsilon_{ijk} \\]\n \\(b_i \\sim N(0,\\sigma_b^2)\\);\n \\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\).\n    parameters explanation    \\(\\beta_j\\) experimental factor (researchers are interested in)  \\(\\sigma_b^2\\) between-rail variability  \\(\\epsilon_{ijk}\\) noise  \\(\\sigma^2\\) within-rail variability     Note this is a balcance design  lmod.random \u0026lt;- lme(score ~ Machine, data = Machines, random = ~1 | Worker) summary(lmod.random) ## Linear mixed-effects model fit by REML ## Data: Machines ## AIC BIC logLik ## 296.8782 306.5373 -143.4391 ## ## Random effects: ## Formula: ~1 | Worker ## (Intercept) Residual ## StdDev: 5.146552 3.161647 ## ## Fixed effects: score ~ Machine ## Value Std.Error DF t-value p-value ## (Intercept) 52.35556 2.229312 46 23.48507 0 ## MachineB 7.96667 1.053883 46 7.55935 0 ## MachineC 13.91667 1.053883 46 13.20514 0 ## Correlation: ## (Intr) MachnB ## MachineB -0.236 ## MachineC -0.236 0.500 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.7248806 -0.5232891 0.1327564 0.6513056 1.7559058 ## ## Number of Observations: 54 ## Number of Groups: 6 Model and assumptions - with interactions \\[ y_{ijk} = \\beta_j + b_i + b_{ij} + \\epsilon_{ijk} \\]\n \\(b_i \\sim N(0,\\sigma_1^2)\\);\n \\(b_{ij} \\sim N(0,\\sigma_2^2)\\);\n \\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\).\n     parameters explanation    \\(\\beta_j\\) experimental factor (researchers are interested in)  \\(b_i\\) blocking factor (source of variablity that is not of the primary interest)  \\(b_{ij}\\) interaction effects between experimental factor and blocking factor  \\(\\sigma_b^2\\) between-rail variability  \\(\\epsilon_{ijk}\\) noise  \\(\\sigma^2\\) within-rail variability     Note this is a balcance design.  # Worker/Machine: Modeling two levels of random effects, Worker and Machine within Worker lmod.random.interact \u0026lt;- lme(score ~ Machine, data = Machines, random = ~1 | Worker/Machine) summary(lmod.random.interact) ## Linear mixed-effects model fit by REML ## Data: Machines ## AIC BIC logLik ## 227.6876 239.2785 -107.8438 ## ## Random effects: ## Formula: ~1 | Worker ## (Intercept) ## StdDev: 4.78105 ## ## Formula: ~1 | Machine %in% Worker ## (Intercept) Residual ## StdDev: 3.729532 0.9615771 ## ## Fixed effects: score ~ Machine ## Value Std.Error DF t-value p-value ## (Intercept) 52.35556 2.485828 36 21.061613 0.0000 ## MachineB 7.96667 2.176972 10 3.659518 0.0044 ## MachineC 13.91667 2.176972 10 6.392672 0.0001 ## Correlation: ## (Intr) MachnB ## MachineB -0.438 ## MachineC -0.438 0.500 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.26958675 -0.54846580 -0.01070594 0.43936568 2.54005792 ## ## Number of Observations: 54 ## Number of Groups: ## Worker Machine %in% Worker ## 6 18 \\(\\beta_1 =52.36\\), \\(\\beta_2 = 52.36 + 7.97\\), … \\(\\sigma_1 = 4.78105\\), \\(\\sigma_2 =3.729532\\), \\(\\sigma = 0.9615771\\).\n Model Selection \\[H_0: \\text{the restricted model} \\qquad H_A: \\text{the general model}\\]\nanova(lmod.random,lmod.random.interact) ## Model df AIC BIC logLik Test L.Ratio ## lmod.random 1 5 296.8782 306.5373 -143.4391 ## lmod.random.interact 2 6 227.6876 239.2785 -107.8438 1 vs 2 71.19063 ## p-value ## lmod.random ## lmod.random.interact \u0026lt;.0001 \\(LR=2[\\log(L_A)-\\log(L_0)] \\sim \\chi^2_{df_A -df_0}\\)\nThe likelihood ratio test suggests that model with interatcions are preferred.\n  Extensions for interaction terms Machines %\u0026gt;% filter(Worker == \u0026quot;1\u0026quot;) %\u0026gt;% {plot.experiment(.$score,.$Machine,.$Worker, lab.x = \u0026quot;Score\u0026quot;, lab.y = \u0026quot;Worker\u0026quot;, lab.experimental = \u0026quot;Machine\u0026quot;)} For each worker, say Worker 1, we previously assume\n\\[ y_{1jk} = \\beta_j + b_1 + b_{1j} + \\epsilon_{1jk} \\]\n \\(b_1 \\sim N(0,\\sigma_1^2)\\);\n \\(b_{1j} \\sim N(0,\\sigma_2^2)\\);\n \\(\\epsilon_{1jk} \\sim N(0,\\sigma^2)\\).\n  We assume \\(b_{1j}\\) iid \\(N(0,\\sigma_2^2)\\), but from the graph above, it seems that it’s not plausible. We can turn to more general variacne structures.\n\\[ \\bf{y_{1}} = X_1\\beta + Z_1b_1 + \\epsilon_{1} \\]\n \\({\\bf{b_1}} \\sim N_3(0,\\Phi)\\);\n \\({\\bf{\\epsilon_{1}}} \\sim N(0,\\sigma^2I_3)\\).\n  To fit this model, we need to specify the \\(X_1\\) and \\(Z_1\\).\n# fixed effect X_1 Machines %\u0026gt;% filter(Worker == \u0026quot;1\u0026quot;) %\u0026gt;% {model.matrix( score ~ Machine, .)} ## (Intercept) MachineB MachineC ## 1 1 0 0 ## 2 1 0 0 ## 3 1 0 0 ## 4 1 1 0 ## 5 1 1 0 ## 6 1 1 0 ## 7 1 0 1 ## 8 1 0 1 ## 9 1 0 1 ## attr(,\u0026quot;assign\u0026quot;) ## [1] 0 1 1 ## attr(,\u0026quot;contrasts\u0026quot;) ## attr(,\u0026quot;contrasts\u0026quot;)$Machine ## [1] \u0026quot;contr.treatment\u0026quot; # fixed effect Z_1 Machines %\u0026gt;% filter(Worker == \u0026quot;1\u0026quot;) %\u0026gt;% {model.matrix( score ~ Machine - 1, .)} ## MachineA MachineB MachineC ## 1 1 0 0 ## 2 1 0 0 ## 3 1 0 0 ## 4 0 1 0 ## 5 0 1 0 ## 6 0 1 0 ## 7 0 0 1 ## 8 0 0 1 ## 9 0 0 1 ## attr(,\u0026quot;assign\u0026quot;) ## [1] 1 1 1 ## attr(,\u0026quot;contrasts\u0026quot;) ## attr(,\u0026quot;contrasts\u0026quot;)$Machine ## [1] \u0026quot;contr.treatment\u0026quot; lmod.random.interact.general \u0026lt;- lme(score ~ Machine, Machines, random = ~ Machine -1 | Worker) summary(lmod.random.interact.general) ## Linear mixed-effects model fit by REML ## Data: Machines ## AIC BIC logLik ## 228.3112 247.6295 -104.1556 ## ## Random effects: ## Formula: ~Machine - 1 | Worker ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## MachineA 4.0792807 MachnA MachnB ## MachineB 8.6252908 0.803 ## MachineC 4.3894795 0.623 0.771 ## Residual 0.9615766 ## ## Fixed effects: score ~ Machine ## Value Std.Error DF t-value p-value ## (Intercept) 52.35556 1.680711 46 31.150834 0.0000 ## MachineB 7.96667 2.420851 46 3.290854 0.0019 ## MachineC 13.91667 1.540100 46 9.036211 0.0000 ## Correlation: ## (Intr) MachnB ## MachineB 0.463 ## MachineC -0.374 0.301 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.39354008 -0.51377575 0.02690829 0.47245472 2.53338699 ## ## Number of Observations: 54 ## Number of Groups: 6 Model Selection\nanova(lmod.random, lmod.random.interact, lmod.random.interact.general) ## Model df AIC BIC logLik Test ## lmod.random 1 5 296.8782 306.5373 -143.4391 ## lmod.random.interact 2 6 227.6876 239.2785 -107.8438 1 vs 2 ## lmod.random.interact.general 3 10 228.3112 247.6295 -104.1556 2 vs 3 ## L.Ratio p-value ## lmod.random ## lmod.random.interact 71.19063 \u0026lt;.0001 ## lmod.random.interact.general 7.37635 0.1173 There’s no evidence to support more general model.\n  ANCOVA with random effects: Orthodont OrthFe \u0026lt;- Orthodont %\u0026gt;% filter(Sex == \u0026quot;Female\u0026quot;) knitr::kable(t(OrthFe)) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;) %\u0026gt;% scroll_box(width = \u0026quot;100%\u0026quot;, height = \u0026quot;100%\u0026quot;)   distance  21.0  20.0  21.5  23.0  21.0  21.5  24.0  25.5  20.5  24.0  24.5  26.0  23.5  24.5  25.0  26.5  21.5  23.0  22.5  23.5  20.0  21.0  21.0  22.5  21.5  22.5  23.0  25.0  23.0  23.0  23.5  24.0  20.0  21.0  22.0  21.5  16.5  19.0  19.0  19.5  24.5  25.0  28.0  28.0    age  8  10  12  14  8  10  12  14  8  10  12  14  8  10  12  14  8  10  12  14  8  10  12  14  8  10  12  14  8  10  12  14  8  10  12  14  8  10  12  14  8  10  12  14    Subject  F01  F01  F01  F01  F02  F02  F02  F02  F03  F03  F03  F03  F04  F04  F04  F04  F05  F05  F05  F05  F06  F06  F06  F06  F07  F07  F07  F07  F08  F08  F08  F08  F09  F09  F09  F09  F10  F10  F10  F10  F11  F11  F11  F11    Sex  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female  Female      Modeling: Random Intercept Model and assumptions:\n\\[ \\bf{y_{i}} = X_i\\beta + Z_ib_i + \\epsilon_{i} \\]\n \\(b_i \\sim N(0,\\sigma^2)\\);\n \\(\\epsilon_{i} \\sim N(0,\\sigma^2I)\\).\n  For example,\n# fixed effect X_1 OrthFe %\u0026gt;% filter(Subject == \u0026quot;F01\u0026quot;) %\u0026gt;% {model.matrix( distance ~ age, .)} ## (Intercept) age ## 1 1 8 ## 2 1 10 ## 3 1 12 ## 4 1 14 ## attr(,\u0026quot;assign\u0026quot;) ## [1] 0 1 # random effect Z_1 OrthFe %\u0026gt;% filter(Subject == \u0026quot;F01\u0026quot;) %\u0026gt;% {model.matrix( distance ~ 1, .)} ## (Intercept) ## 1 1 ## 2 1 ## 3 1 ## 4 1 ## attr(,\u0026quot;assign\u0026quot;) ## [1] 0 lmod.OrthFe \u0026lt;- lme(distance ~ age, data=OrthFe, random = ~ 1 | Subject) summary(lmod.OrthFe) ## Linear mixed-effects model fit by REML ## Data: OrthFe ## AIC BIC logLik ## 149.2183 156.169 -70.60916 ## ## Random effects: ## Formula: ~1 | Subject ## (Intercept) Residual ## StdDev: 2.06847 0.7800331 ## ## Fixed effects: distance ~ age ## Value Std.Error DF t-value p-value ## (Intercept) 17.372727 0.8587419 32 20.230440 0 ## age 0.479545 0.0525898 32 9.118598 0 ## Correlation: ## (Intr) ## age -0.674 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.2736479 -0.7090164 0.1728237 0.4122128 1.6325181 ## ## Number of Observations: 44 ## Number of Groups: 11  Modeling: Random Intercept Model and assumptions:\n\\[ \\bf{y_{i}} = X_i\\beta + Z_i{\\bf b_i} + \\epsilon_{i} \\]\n \\(b_i \\sim N_2(0,\\Phi)\\);\n \\(\\epsilon_{i} \\sim N(0,\\sigma^2I)\\).\n  Here \\(b_i=(b_{i1},b_{i2})\\) and \\(Z_i\\) is identical to \\(X_i\\).\nlmod.OrthFe.slope \u0026lt;- lme(distance ~ age, data=OrthFe, random = ~ age | Subject) summary(lmod.OrthFe.slope) ## Linear mixed-effects model fit by REML ## Data: OrthFe ## AIC BIC logLik ## 149.4287 159.8547 -68.71435 ## ## Random effects: ## Formula: ~age | Subject ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 1.8841866 (Intr) ## age 0.1609278 -0.354 ## Residual 0.6682746 ## ## Fixed effects: distance ~ age ## Value Std.Error DF t-value p-value ## (Intercept) 17.372727 0.7606027 32 22.840737 0 ## age 0.479545 0.0662140 32 7.242353 0 ## Correlation: ## (Intr) ## age -0.637 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -1.85438224 -0.46784889 0.06779759 0.42976634 1.59215840 ## ## Number of Observations: 44 ## Number of Groups: 11 anova(lmod.OrthFe, lmod.OrthFe.slope) ## Model df AIC BIC logLik Test L.Ratio ## lmod.OrthFe 1 4 149.2183 156.1690 -70.60916 ## lmod.OrthFe.slope 2 6 149.4287 159.8547 -68.71435 1 vs 2 3.789622 ## p-value ## lmod.OrthFe ## lmod.OrthFe.slope 0.1503 This result implies there’s no explicit evidence to support the random slope model.\n Predicitons  knitr::kable(t(random.effects(lmod.OrthFe))) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;) %\u0026gt;% scroll_box(width = \u0026quot;100%\u0026quot;, height = \u0026quot;100%\u0026quot;)    F10  F09  F06  F01  F05  F07  F02  F08  F03  F04  F11      (Intercept)  -4.005329  -1.470449  -1.470449  -1.229032  -0.021947  0.3401786  0.3401786  0.7023042  1.06443  2.150807  3.599309        Nested Classification Factors: Pixel knitr::kable(t(Pixel)) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;) %\u0026gt;% scroll_box(width = \u0026quot;100%\u0026quot;, height = \u0026quot;100%\u0026quot;)    1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  101  102      Dog  1  1  1  1  1  1  1  2  2  2  2  2  2  2  3  3  3  3  3  3  3  4  4  4  4  4  4  4  5  5  5  5  5  6  6  6  6  6  7  7  7  7  8  8  8  8  9  9  10  10  10  1  1  1  1  1  1  1  2  2  2  2  2  2  2  3  3  3  3  3  3  3  4  4  4  4  4  4  4  5  5  5  5  5  6  6  6  6  6  7  7  7  7  8  8  8  8  9  9  10  10  10    Side  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  R  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L  L    day  0  1  2  4  6  10  14  0  1  2  4  6  10  14  0  4  6  8  10  14  21  0  4  6  8  10  14  21  4  6  8  10  14  4  6  8  10  14  4  6  10  14  4  6  10  14  4  8  4  6  8  0  1  2  4  6  10  14  0  1  2  4  6  10  14  0  4  6  8  10  14  21  0  4  6  8  10  14  21  4  6  8  10  14  4  6  8  10  14  4  6  10  14  4  6  10  14  4  8  4  6  8    pixel  1045.8  1044.5  1042.9  1050.4  1045.2  1038.9  1039.8  1041.8  1045.6  1051.0  1054.1  1052.7  1062.0  1050.8  1039.8  1082.9  1091.1  1088.5  1099.4  1086.8  1073.6  1034.1  1090.7  1126.4  1108.8  1122.5  1100.7  1102.2  1124.4  1128.2  1124.1  1126.7  1109.9  1094.2  1084.1  1093.9  1082.7  1076.1  1151.3  1160.3  1139.7  1133.2  1077.5  1085.9  1100.8  1083.4  1068.2  1064.0  1095.0  1094.7  1081.4  1040.9  1054.1  1051.9  1054.1  1051.3  1051.4  1046.1  1053.6  1052.9  1053.6  1057.1  1064.0  1065.3  1053.4  1043.9  1069.0  1082.5  1077.7  1073.5  1076.4  1053.8  1039.9  1114.1  1111.5  1100.3  1124.0  1113.2  1077.0  1114.7  1117.6  1128.2  1130.5  1102.2  1106.0  1121.5  1115.5  1105.2  1092.8  1129.7  1136.2  1108.5  1094.3  1098.8  1103.6  1106.8  1105.0  1097.2  1099.5  1132.3  1154.5  1161.1      plot(Pixel) Modeling Model and assumptions\n\\[ y_{ijk} = \\beta_1 + \\beta_2 d_{ik} + b_{i1} + b_{i2}d_{i,k} + b_{ij} + \\epsilon_{ijk} \\]\n \\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\).    parameters explanation    \\(\\beta_1\\) intercept  \\(\\beta_2\\) slope  \\(\\sigma_b^2\\) between-rail variability  \\(\\epsilon_{ijk}\\) noise  \\(\\sigma^2\\) within-rail variability      observation explanation    \\(y_{ijk}\\) the k-th observation on the i-th dog’s j-th side  \\(d_{ik}\\) k-th obsservation on i-th dog     \\(b_{i1}\\): random shift on intercept \\(b_{i2}\\): random shift on slope \\(b_{ij}\\): random shift on intercept for side within dog  lmod.nested \u0026lt;- lme(fixed = pixel ~ day + I(day^2), data=Pixel, random = list(Dog = pixel ~ day, Side = pixel ~ 1)) summary(lmod.nested) ## Linear mixed-effects model fit by REML ## Data: Pixel ## AIC BIC logLik ## 841.2102 861.9712 -412.6051 ## ## Random effects: ## Formula: pixel ~ day | Dog ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 28.36990 (Intr) ## day 1.84375 -0.555 ## ## Formula: pixel ~ 1 | Side %in% Dog ## (Intercept) Residual ## StdDev: 16.82431 8.989606 ## ## Fixed effects: pixel ~ day + I(day^2) ## Value Std.Error DF t-value p-value ## (Intercept) 1073.3391 10.171686 80 105.52225 0 ## day 6.1296 0.879321 80 6.97083 0 ## I(day^2) -0.3674 0.033945 80 -10.82179 0 ## Correlation: ## (Intr) day ## day -0.517 ## I(day^2) 0.186 -0.668 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.8290572 -0.4491811 0.0255493 0.5572163 2.7519651 ## ## Number of Observations: 102 ## Number of Groups: ## Dog Side %in% Dog ## 10 20 intervals(lmod.nested) ## Approximate 95% confidence intervals ## ## Fixed effects: ## lower est. upper ## (Intercept) 1053.0968388 1073.3391382 1093.5814376 ## day 4.3796925 6.1295971 7.8795016 ## I(day^2) -0.4349038 -0.3673503 -0.2997967 ## attr(,\u0026quot;label\u0026quot;) ## [1] \u0026quot;Fixed effects:\u0026quot; ## ## Random Effects: ## Level: Dog ## lower est. upper ## sd((Intercept)) 15.9292099 28.3699038 50.5267650 ## sd(day) 1.0814073 1.8437505 3.1435110 ## cor((Intercept),day) -0.8943751 -0.5547222 0.1906591 ## Level: Side ## lower est. upper ## sd((Intercept)) 10.41733 16.82431 27.17176 ## ## Within-group standard error: ## lower est. upper ## 7.634529 8.989606 10.585199 plot(augPred(lmod.nested)) lmod.nested.res1 \u0026lt;- lme(fixed = pixel ~ day + I(day^2), data=Pixel, random = ~ day|Dog) summary(lmod.nested.res1) ## Linear mixed-effects model fit by REML ## Data: Pixel ## AIC BIC logLik ## 884.5196 902.6854 -435.2598 ## ## Random effects: ## Formula: ~day | Dog ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 29.883240 (Intr) ## day 1.754207 -0.489 ## Residual 14.056243 ## ## Fixed effects: pixel ~ day + I(day^2) ## Value Std.Error DF t-value p-value ## (Intercept) 1072.9283 10.445666 90 102.71516 0 ## day 6.0889 1.146997 90 5.30856 0 ## I(day^2) -0.3568 0.052210 90 -6.83314 0 ## Correlation: ## (Intr) day ## day -0.541 ## I(day^2) 0.286 -0.799 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.85494002 -0.53431941 -0.01425967 0.56188163 2.81513852 ## ## Number of Observations: 102 ## Number of Groups: 10 anova(lmod.nested.res1, lmod.nested) ## Model df AIC BIC logLik Test L.Ratio ## lmod.nested.res1 1 7 884.5196 902.6854 -435.2598 ## lmod.nested 2 8 841.2102 861.9712 -412.6051 1 vs 2 45.3094 ## p-value ## lmod.nested.res1 ## lmod.nested \u0026lt;.0001 lmod.nested.res2 \u0026lt;- lme(fixed = pixel ~ day + I(day^2), data=Pixel, random = ~ 1|Dog/Side) anova(lmod.nested.res2, lmod.nested) ## Model df AIC BIC logLik Test L.Ratio ## lmod.nested.res2 1 6 876.8390 892.4098 -432.4195 ## lmod.nested 2 8 841.2102 861.9712 -412.6051 1 vs 2 39.62885 ## p-value ## lmod.nested.res2 ## lmod.nested \u0026lt;.0001   Split-plot plot(Oats,inner=Oats$Variety)  ","date":1521158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528243200,"objectID":"19998979e23a7b5548bded2a61b48d77","permalink":"/post/explore-the-linear-mixed-effect-model/","publishdate":"2018-03-16T00:00:00Z","relpermalink":"/post/explore-the-linear-mixed-effect-model/","section":"post","summary":"Basic concepts and R commands to play with LME.","tags":["LME"],"title":"Explore the Linear Mixed Effect Model","type":"post"},{"authors":null,"categories":null,"content":"In this project, we\n Develope data products to help Airbnb hosts to determine listing prices using Sparse Regression and RandomForest Researched how amenities and geolocation in uence listing prices Designed a User Interface for customers to gain insight into Airbnb rental markets in Boston  The Video Presentation and Rshiny App Demo are also provided.\n ","date":1512882000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512882000,"objectID":"690845e8dc7a3b2b3563f360194fbc75","permalink":"/project/boston-housing/","publishdate":"2017-12-10T00:00:00-05:00","relpermalink":"/project/boston-housing/","section":"project","summary":"Develope data products to help Airbnb hosts to determine listing prices.","tags":["Data Analysis"],"title":"Real Estate Market Data Analysis","type":"project"},{"authors":null,"categories":null,"content":"This is the deep learning course final project trying to reporduce the results reported in the paper, Show and Tell: A Neural Image Caption Generator.\nThe model is trained on the MS coco2014 dataset. Our final result looks like\nFor code, UI, and report please click here.\n","date":1512882000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512882000,"objectID":"217bfb70762bb4746b8027e79602a247","permalink":"/project/show-and-tell/","publishdate":"2017-12-10T00:00:00-05:00","relpermalink":"/project/show-and-tell/","section":"project","summary":"Feed in an image, AI will generate the caption for you!","tags":["Deep Learning","CNN","LSTM"],"title":"Show and Tell: A Neural Image Caption Generator","type":"project"},{"authors":null,"categories":null,"content":"This is the statistical computing course final project, trying to understand, reporduce and extend some results reported in the paper, Variational Inference: A Review for Statisticians.\nThree datasets are used here, simulated data, old faithful and imageCLEF.\nOur final result looks like\n Simulated data   old faithful  imageclef\n  For code and report please click here.\n","date":1512882000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512882000,"objectID":"ef4fca63f9151ea900f34782ac9bfb47","permalink":"/project/variational-inference/","publishdate":"2017-12-10T00:00:00-05:00","relpermalink":"/project/variational-inference/","section":"project","summary":"Clustering under the variation inference framework.","tags":["Data Analysis"],"title":"Variational Gaussian Mixtures","type":"project"},{"authors":null,"categories":["optimization"],"content":"  Problem description Notations Assumption Algorithm  Convergence Analysis Powell’s example R codes for numerical experiments   We mainly focus on the convergence of Block coordinate decent with exact minimization, whose block update strategy employs Gauss-Seidel manner. And then use Powell’s example to see what will happen if some conditions are not met.\n Reference: 1. Dimitri .P Bertsekas, Nonlinear Programming 2ed 2. Powell ,1973, ON SEARCH DIRECTIONS FOR MINIMIZATION ALGORITHMS\n Problem description Notations We want to solve the problem:\n\\[\\mathop{min}_{x\\in X}\\quad f(x)\\]\nwhere X is a Cartesian product of closed convex sets $X_1,…,X_m:X=_{i=1}^n X_i $\nWe assume that \\(X_i\\) is a closed convex subset of \\(R^{n_i}\\) and \\(n=\\sum_{i=1}^m n_i\\). The vector is partitioned into \\(m\\) block(s) such that \\(x_i \\in X^{n_i}\\).\nWe denote \\(\\nabla_i f\\) as the gradient of \\(f\\) with respect to component \\(x_i\\).\n Assumption We shall assume that for every \\(x\\in X\\) and \\(i=1,2,...m\\) the optimization problem\n\\[\\mathop{min}_{\\xi\\in X_i}\\quad f(x_1,...,x_{i-1},\\xi,x_{i+1,....,x_m})\\]\nhas at least one solution.\n Algorithm The Gauss-Seidel method, generates the next iterate \\(x^{k+1}=(x^{k+1}_1,...,x^{k+1}_m)\\), given the current the iterate \\(x^{k}=(x^{k}_1,...,x^{k}_m)\\), according to the iteration\n\\[x^{k+1}_i=\\mathop{argmin}_{\\xi\\in X_i}\\quad f(x_1^{k+1},...,x^{k+1}_{i-1},\\xi,x^k_{i+1},...,x_m^k)\\]\n  Convergence Analysis Theorem Suppose that \\(f\\) is continuously differentiable over the set \\(X\\) defined as above. Furthermore, suppose that for each \\(i\\) and \\(x\\in X\\),\n\\[f(x_1,...,x_{i-1},\\xi,x_{i+1,....,x_m})\\]\nviewed as a function of \\(\\xi\\), attains a unique minimum \\(\\bar x_i\\) over \\(X_i\\) and is monotonically non-increasing in the interval from \\(x_i\\) to \\(\\bar \\xi\\). Let \\(\\{x_k\\}\\) be the sequence generated by the block coordinate method with Gauss-Seidel manner. Then, every limit point of \\(\\{x_k\\}\\) is a stationary point.\nPROOF\nLet\n\\[z_i^k=(x_1^{k+1},...,x_i^{k+1},x_{i+1}^k,...,x_m^k)\\]\nBy the nature of this algorithm, for all \\(k\\geq 0\\), we have following inequality\n\\[f(x^k)\\geq f(z_1^k)\\geq f(z_2^k)\\geq ...\\geq f(z_{m-1}^k)\\geq f(x^{k+1}) \\quad (*)\\]\nSince \\(\\{x_k\\}in X\\), we can assume \\(\\{x^{k_j}\\}\\) is the subsequence that converges to \\(\\bar x=(\\bar x_1,..,\\bar x_m)\\).\nNow we want prove that \\(\\bar x\\) is the stationary point of \\(f\\).\nFrom (*), we know that\n\\[f(z_1^{k_j})\\leq f(x_1,x_2^{k_j},..., x_m^{k_j})\\qquad \\forall x_1\\in X_1\\]\nLet \\(j\\rightarrow +\\infty\\), we derive\n\\[f(\\bar x)\\leq f(x_1,\\bar x_2,..., \\bar x_m)\\overset \\Delta = h(x_1)\\qquad \\forall x_1\\in X_1\\]\nwhich implies that \\(\\bar x_i\\) is the minima of \\(h(x_1)\\) on \\(X_1\\). Using the optimality over a convex set, we conclude that\n\\[h\u0026#39;(\\bar x_1)(\\bar x_1 -x_1)\\geq 0 \\Leftrightarrow (x_1-\\bar x_1)^T\\nabla_1f(\\bar x_1)\\geq 0\\qquad x_1\\in X_1\\]\nAt this stage, if we can prove that \\(\\{z_1^{k_j}\\}\\) converges to \\(\\bar x\\), we can show that\n\\[ (x_2-\\bar x_2)^T\\nabla_2 f(\\bar x_2)\\geq 0\\qquad x_2\\in X_2\\], since\n\\[f(z_1^{k_j})=f(x_1^{k_j+1},x_2^{k_j},x_3^{k_j},...,x_m^{k_j})\\leq f(x_1^{k_j+1},x_2,x_3^{k_j},...,x_m^{k_j})\\qquad x_2\\in X_2\\]\nLet \\(j\\rightarrow +\\infty\\), we derive\n\\[f(\\bar x)\\leq f(\\bar x_1,\\bar x_2,\\bar x_3,..., \\bar x_m)\\qquad \\forall x_2\\in X_2\\]\nand\n\\[(x_2-\\bar x_2)^T\\nabla_2f(\\bar x_2)\\geq 0\\qquad x_2\\in X_2\\]\n(Note: Although \\(x_1^{k_j+1}\\) may not in the sequence \\(\\{x_1^{k_t}\\}_{t\\geq 1}\\) ,which convergences to \\(\\bar x_1\\), but \\(\\{z_1^{k_j}\\}\\) converges to \\(\\bar x\\), so its component \\(x_1^{k_j+1}\\) converges to \\(\\bar x_1\\)).\nFurthermore, if we prove that for \\(i=1,2,...,m-1\\),\\(\\{z_i^{k_j}\\}\\) convergences to \\(\\bar x\\), then we have\n\\[(x_i-\\bar x_i)^T\\nabla_i\\;f(\\bar x_i)\\geq 0\\qquad x_i\\in X_i\\]\nAnd thus \\(\\bar x\\) is a stationary point, since \\((x-\\bar x)^T\\nabla f(\\bar x)\\geq 0\\)\nBy far, it remains to prove that \\(\\{z_i^{k_j}\\}\\quad,\\forall i\\) convergence to \\(\\bar x\\). First,we try to prove that \\(\\{z_1^{k_1}\\}\\) convergence to \\(\\bar x\\).\nAssume the contrary that $r{k_j}=z_1{k_j}-x^{k_j}$ doesn’t convergence to 0. Let \\(s_1^{k_j}=(z_1^{k_j}-x^{k_j})/r^{k_j}\\). Thus, \\(z_1^{k_j}=x^{k_j}+r^{k_j}s_1^{k_j}\\) , \\(\\vert \\vert r_{k_j}\\vert \\vert =1\\) and \\(s_1^{k_j}\\) differs from 0 only along the first block-component. Since \\(\\{s_1^{k_j}\\}\\) belong to a compact set and therefore without loss of generality, we assume \\(s_1^{k_j}\\) convergences to \\(\\bar s_1\\).\nSince \\(r^{k_j}\u0026gt;0\\),we can find a \\(\\epsilon\\in (0,1)\\), such that \\(x^{k_j}+\\epsilon s_1^{k_j}\\) lies on the segment joining \\(x^{k_j}\\) and \\(x^{k_j}+s_1^{k_j}=z_1^{k_j}\\). Using the non-increasing property of \\(f\\),we derive,\n\\[f(z_1^{k_j})\\leq f(x^{k_j}+\\epsilon s_1^{k_j}) \\leq f(x^{k_j})\\]\nAgain, using (*), we conclude\n\\[f(x^{k_{j+1}})\\leq f(z_1^{k_j})\\leq f(x^{k_j}+\\epsilon s_1^{k_j}) \\leq f(x^{k_j})\\]\nLet \\(j\\rightarrow +\\infty\\), we derive \\(f(\\bar x)=f(\\bar x+\\epsilon \\bar s_1)\\), which contradicts the hypothesis that \\(f\\) is uniquely minimized when viewed as a function of the first block component. This contradiction establishes that \\(\\{z_1^{k_1}\\}\\) convergence to \\(\\bar x\\).\nSimilarly, let $r_t{k_j}=z_t{k_j}-z_{t-1}^{k_j}$ for \\(t=2,3,...,m-1\\) and using the same technique shown above, we finally prove that \\(\\{z_i^{k_j}\\},\\quad \\forall i\\).\n Powell’s example In ON SEARCH DIRECTIONS FOR MINIMIZATION ALGORITHMS, Power actually gives three examples that sequences generated by the algorithm discussed above do not convergence to stationary points once some hypothesis are not met.\n The first example is straightforward, However, the remarkable properties of this example can be destroyed by making a small perturbation to the starting vector \\(x^0\\).\n The second example is not sensitive to either small changes in the initial data or to small errors introduced during the iterative process, for example computer rounding errors.\n The third example suggests that a function that is infinitely differentiable that also causes an endless loop in the iterative minimization method.\n  We here only presents the first example. Consider the following function\n\\[f(x,y,z)=-(xy+yz+zx)+(x-1)_+^2+(-x-1)_+^2+(y-1)_+^2+(-y-1)_+^2+(z-1)_+^2+(-z-1)_+^2\\]\nwhere\n\\[(x-c)_+^2=\\begin{cases}0,x-c\u0026lt; 0\\\\ (x-c)^2,x-c\\geq 0\\end{cases}\\]\nGiven the starting point \\(x_0=(-1-e,1+\\frac{1}{2}e,-1-\\frac{1}{4}e)\\) and use block coordinate decent algorithm,and we update the variable in a manner of \\(x\\rightarrow y\\rightarrow z\\rightarrow x ...\\) with\n\\[x_{k+1}^{**}\\leftarrow sign(y_k+z_k)[1+\\frac{1}{2}\\vert y_k+z_k\\vert ]\\]\n\\[y_{k+1}^{**}\\leftarrow sign(x_{k+1}+z_k)[1+\\frac{1}{2}\\vert x_{k+1}+z_k\\vert ]\\]\n\\[z_{k+1}^{**}\\leftarrow sign(x_{k+1}+y_{k+1})[1+\\frac{1}{2}\\vert x_{k+1}+y_{k+1}\\vert ]\\]\nWe here present the first six steps of this case\n  cycle/totall iteration x y z    1/1 1+\\(\\frac{1}{8}e\\) 1+$e $ -1-\\(\\frac{1}{4}e\\)  1/2 1+\\(\\frac{1}{8}e\\) -1-\\(\\frac{1}{16}e\\) -1-\\(\\frac{1}{4}e\\)  1/3 1+\\(\\frac{1}{8}e\\) -1-\\(\\frac{1}{16}e\\) 1+\\(\\frac{1}{32}e\\)  2/4 -1-\\(\\frac{1}{64}e\\) -1-\\(\\frac{1}{16}e\\) 1+\\(\\frac{1}{32}e\\)  2/5 -1-\\(\\frac{1}{64}e\\) 1+\\(\\frac{1}{128}e\\) 1+\\(\\frac{1}{32}e\\)  2/6 -1-\\(\\frac{1}{64}e\\) 1+\\(\\frac{1}{128}e\\) -1-\\(\\frac{1}{256}e\\)  3/7 1+\\(\\frac{1}{512}e\\) 1+\\(\\frac{1}{128}e\\) -1-\\(\\frac{1}{256}e\\)  … … … …    This result implies that the sequence obtained by this algorithm can not converge to one single point since \\(x-coordinate\\) change its sign as the even cycle and odd cycle alternate. Situations are similar for \\(y-coordinate\\) and \\(z-coordinate\\).\nBut \\(\\{x_k\\}\\) has six sub-sequences which convergence to (1,1,-1), (1,-1,-1), (1,-1,1), (-1,-1,1),(-1,-1,1),(-1,1,1),(-1,1,-1) respectively.\n Remark\nA hint to derive the update formula:\n\\[x\\leftarrow sign(y+z)[1+\\frac{1}{2}(y+z)]\\]\nIndeed, derivates of \\((x-1)_+^2\\) and \\((-x-1)_+^2\\) are as follows respecively\n\\[\\frac{d(x-1)_+^2}{dx}=\\begin{cases}2(x-1),x\\geq 1\\\\0,x\u0026lt;1\\end{cases}\\quad \\frac{d(-x-1)_+^2}{dx}=\\begin{cases}2(-x-1),x\\leq -1\\\\0,x\u0026gt;-1\\end{cases} \\]\nSo for the univariate optimization problem, setting the derivate of \\(g(x)=f(x,y,z)\\) to zero, we conclude\n\\[\\frac{\\partial f(x,y,x)}{\\partial x}=0\\Rightarrow \\begin{cases}x\\geq 1: x=1+\\frac{1}{2}(y+z)\\\\-1\u0026lt; x\u0026lt;1: -(y+z)=0\\\\x\\leq -1:x=-1+\\frac{1}{2}(y+z) \\end{cases}\\]\n The gradient of \\(f(x,y,z)\\) on this cyclic path, is \\(\\nabla f(x,y,z)=(-y-z,-x-z,-x-y)\\) and \\(\\vert \\vert \\nabla f(x,y,z)\\vert \\vert _1=2\\)\n This example is unstable with respect to small perturbations. Small changes in the starting point \\(x_0=(-1-e,1+\\frac{1}{2}e,-1-\\frac{1}{4}e)\\) or smal errors in the numbers that are computed during the calculation will destroy the cyclic behavior.\nIt’s s clear the choice of perturbations \\(e\\) plays a key role. Say, \\(x_0=(-1-e_1,1+e_2,-1-e_3)\\) and we have \\(e_k=\\frac{1}{2}(e_{k-2}- e_{k-1})\\)\n  cycle/totall iteration x y z    1/1 1+\\(e_4\\) 1+\\(e_2\\) -1-\\(e_3\\)  1/2 1+\\(e_4\\) -1-\\(e_5\\) -1-\\(e_3\\)  1/3 1+\\(e_4\\) -1-\\(e_5\\) 1+\\(e_6\\)  2/4 -1-\\(e_7\\) -1-\\(e_5\\) 1+\\(e_6\\)  2/5 -1-\\(e_7\\) 1+\\(e_8\\) 1+\\(e_6\\)  2/6 -1-\\(e_7\\) 1+\\(e_8\\) -1-\\(e_9\\)  … … … …    To preserve the cyclic behavior , we have to make sure that \\(e_{k-2}\u0026gt;e_{k-1}\\)\nAnd in practice, when we do some numerical tests, we shall find that, this theoretically-existed endless loop actual breaks down due to the rounding errors. A brief illustration is given below. In this experiment, loop ends at the 52 steps.\nSnip20161117_15\n As\n\\[\\frac{\\partial f(x,y,x)}{\\partial x}=0\\Rightarrow \\begin{cases}x\\geq 1: x=1+\\frac{1}{2}(y+z)\\\\-1\u0026lt; x\u0026lt;1: -(y+z)=0\\\\x\\leq -1:x=-1+\\frac{1}{2}(y+z) \\end{cases}\\]\nsuggests that, when \\(-1\u0026lt;x\u0026lt;1\\), the choice of \\(x\\) is arbitrary and we set \\(x^*=0\\) in the case above. So the uniqueness requirement is violated. It turns out that the six vertices are even not the stationary points.\nFor example, at point \\(\\bar x=(1,1,-1)\\), \\(\\nabla f(\\bar x)=(0,0,-2)\\) and for any ponit \\(x\\) in the unit cubic \\((x-\\bar x)^T\\nabla f(\\bar x)\\leq 0\\). Say, \\(x=(0.9,0.9,-0.9)\\), \\((x-\\bar x)^T\\nabla f(\\bar x)=-0.2\u0026lt;0\\)\nActually, as in the proof of Theorem, we prove that \\(\\{z_1^{k_j}\\}\\) converges to \\(\\bar x\\), where \\(\\bar x\\) is the limit point of \\(\\{x^{k_j}\\}\\). But in this example, the limit point of \\(\\{z_1^{k_j}\\}\\) is (1,1,-1) while the limit point of \\(\\{x^{k_j}\\}\\) is either (-1,1,-1) or (1,-1,1). So the requirement of uniqueness is not met.\n   R codes for numerical experiments #################### ### Function for test ### #################### PowellE1\u0026lt;-function(xstart,cycles,fig=T){ #######function part ############## UpdateCycle\u0026lt;-function(x){ Sign\u0026lt;-function(x){ if (x\u0026gt;0){ return(1) }else{ if (x\u0026lt;0){ return(-1) }else{ return(0) } } } x.new\u0026lt;-c() x.new[1]\u0026lt;-Sign(x[2]+x[3])*(1+0.5*abs(x[2]+x[3])) x.new[2]\u0026lt;-Sign(x.new[1]+x[3])*(1+0.5*abs(x.new[1]+x[3])) x.new[3]\u0026lt;-Sign(x.new[1]+x.new[2])*(1+0.5*abs(x.new[1]+x.new[2])) cycle\u0026lt;-matrix(c(x.new[1],x[2],x[3],x.new[1],x.new[2],x[3],x.new[1],x.new[2],x.new[3]), ncol=3,byrow=T) return(cycle) } fpowell\u0026lt;-function(x){ PostivePart\u0026lt;-function(x){ ifelse(x\u0026gt;=0,x,0) } fval\u0026lt;-(-(x[1]*x[2]+x[2]*x[3]+x[1]*x[3]))+ PostivePart(x[1]-1)^2+PostivePart(-x[1]-1)^2+ PostivePart(x[2]-1)^2+PostivePart(-x[2]-1)^2+ PostivePart(x[3]-1)^2+PostivePart(-x[3]-1)^2 return(fval) } ############ operation part ################ x.store\u0026lt;-matrix(ncol=3,nrow=cycles*3+1) x.store[1,]\u0026lt;-xstart for (i in seq_len(cycles)){ x.store[(3*i-1):(3*i+1),]\u0026lt;-UpdateCycle(x.store[3*i-2,]) } x.store\u0026lt;-x.store[-1,] fval\u0026lt;-rep(0,cycles*3) for(i in seq_len(cycles*3)){ fval[i]\u0026lt;-fpowell(x.store[i,]) } fval\u0026lt;-as.matrix(fval) if (fig==T){ plot(fval,ylim=c(min(fval)-1,max(fval)+1),type=\u0026quot;l\u0026quot;,xlab=\u0026quot;Iterations\u0026quot;,ylab = \u0026quot;F value\u0026quot;) } r\u0026lt;-list() r$x.iterate\u0026lt;-x.store r$fval\u0026lt;-fval return(r) } ################## #### Test 1 ######## ################## perturb\u0026lt;-0.5 xstart\u0026lt;-c(-1-perturb,1+0.5*perturb,-1-0.25*perturb) cycles\u0026lt;-20 r\u0026lt;-PowellE1(xstart,cycles,fig=T) ################## #### Test 2 ######## ################## perturb\u0026lt;-0.5 xstart\u0026lt;-c(-1-perturb,1+0.5*perturb,-1-0.25*perturb) cycles\u0026lt;-20 r\u0026lt;-PowellE1(xstart,cycles,fig=T) ################## #### Test 3 ######## ################## xstart\u0026lt;-c(3,2,1) cycles\u0026lt;-100 r\u0026lt;-PowellE1(xstart,cycles,fig=T)  ","date":1479340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479340800,"objectID":"18229ae12d624502e0267d2ebc3c1198","permalink":"/post/convergence-analysis-for-block-coordinate-descent-algorithm-and-powells-examples/","publishdate":"2016-11-17T00:00:00Z","relpermalink":"/post/convergence-analysis-for-block-coordinate-descent-algorithm-and-powells-examples/","section":"post","summary":"Convergence analysis of Block coordinate decent with exact minimization.","tags":["Powell-Example"],"title":"Convergence Analysis for Bloock Coordinate Decent Algorithm and Powell's Examples","type":"post"}]